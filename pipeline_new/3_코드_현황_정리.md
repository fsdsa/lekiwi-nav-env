# 3-Skill Pipeline — 전체 코드 현황 정리

> 작성일: 2026-02-20 (갱신: AUDIT_REPORT + INDEPENDENT_AUDIT_REPORT + FEEDBACK 수정 반영)
> 기준: `feedback/AUDIT_REPORT.md` + `feedback/INDEPENDENT_AUDIT_REPORT.md` 전체 수정 완료 후 상태
> 이 문서는 파이프라인의 전체 구조와 모든 코드 파일의 완전한 소스를 포함합니다.

---

## 1. 전체 아키텍처

### 1-1. 3-Skill Pipeline v2 개요

```
┌─────────────────────────────────────────────────────────────────────┐
│                     3-Skill Pipeline v2                             │
│                                                                     │
│  Skill-1: Navigate     Skill-2: ApproachAndGrasp  Skill-3: CarryAndPlace │
│  (Script Policy)       (RL: AAC-PPO)              (RL: AAC-PPO)     │
│  collect_navigate_     lekiwi_skill2_env.py        lekiwi_skill3_env.py  │
│  data.py               30D actor / 37D critic      29D actor / 36D critic│
│                        Action: [arm5,grip1,base3]  Action: [arm5,grip1,base3]│
│                              │                           ▲            │
│                              │   generate_handoff_       │            │
│                              └──▶ buffer.py ────────────┘            │
│                                   (pickle)                           │
└─────────────────────────────────────────────────────────────────────┘
```

### 1-2. 데이터 파이프라인

```
텔레옵(리더암+키보드)          RL Expert              Script Policy
       │                         │                         │
  record_teleop.py          collect_demos.py        collect_navigate_data.py
       │                         │                         │
       └────────┬────────────────┘                         │
                ▼                                          │
           HDF5 (obs, actions, robot_state, images)        │
                │                                          │
           train_bc.py (BC 학습, obs_dim 필수)             │
                │                                          │
           bc_nav.pt ──▶ train_lekiwi.py (PPO + BC warm-start)
                                │
                         best_agent.pt
                                │
                    collect_demos.py (expert rollout)
                                │
                   HDF5 + camera images
                                │
                  convert_hdf5_to_lerobot_v3.py
                                │
                   LeRobot v3 dataset → VLA 파인튜닝
```

### 1-3. Action 순서 (lekiwi_v6)

모든 v2 스킬에서 동일:
```
action[0:5]  = arm joint position target (5D)
action[5]    = gripper position target (1D)
action[6:8]  = base linear velocity vx, vy (2D)
action[8]    = base angular velocity wz (1D)
```

### 1-4. 핵심 설계 결정

| 항목 | 설계 |
|------|------|
| Body displacement | `root_lin_vel_b` / `root_ang_vel_b` 직접 읽기 (m/s, rad/s) |
| Curriculum | 시작 0.5m → 성공률 70% 초과 시 0.25m 증가 |
| break_force | 30N (mass×g×10), DR 15~45N |
| Gripper | RL: continuous, VLA: binary (threshold 0.5) |
| Base 단위 | sim=m/s, VLA=m/s (변환 불필요) |
| Handoff | Skill-2 성공 상태 → pickle → Skill-3 초기 상태 |
| Gripper body | `"Moving_Jaw_08d_v1"` via `find_bodies()` |
| Grasp break | gripper body_pos_w vs object_pos_w > 0.15m → drop |
| AAC | skrl 1.4.3 네이티브 미지원 → `aac_wrapper.py` + `aac_ppo.py` + `aac_trainer.py` 3파일로 구현 |
| Intentional Place | Skill-3: gripper_open + near_home → FixedJoint 해제 (just_dropped=False 유지, place_success 달성 가능) |

---

## 2. 파일 구조

### 2-1. 새로 생성된 파일 (3-Skill 전용)

| 파일 | 줄수 | 역할 |
|------|------|------|
| `lekiwi_skill2_env.py` | 1647 | Skill-2 ApproachAndGrasp 환경 |
| `lekiwi_skill3_env.py` | 494 | Skill-3 CarryAndPlace 환경 (intentional place + CRITICAL-2 fix + batched reset) |
| `generate_handoff_buffer.py` | 176 | Handoff Buffer 생성기 (AAC 지원) |
| `collect_navigate_data.py` | 601 | Skill-1 Navigate 데이터 수집 |
| `calibrate_tucked_pose.py` | — | Tucked Pose 측정 (리더암 TCP, self-collision 방지 한계) |
| `calibrate_arm_limits.py` | — | Arm Joint Limits 측정 (리더암 TCP, 관절별 min/max) |
| `aac_wrapper.py` | 45 | AAC wrapper — skrl IsaacLabWrapper에 state() 노출 |
| `aac_ppo.py` | 358 | AAC PPO agent — critic에 별도 privileged obs 전달 |
| `aac_trainer.py` | 94 | AAC trainer — env.state()로 critic obs 추적 |

### 2-2. 기존 파일 (수정됨)

| 파일 | 줄수 | 수정 내용 |
|------|------|-----------|
| `models.py` | 115 | CriticNet 추가 (AAC용) |
| `train_lekiwi.py` | 526 | `--skill` 분기, BC warm-start, **AAC 통합** |
| `train_bc.py` | 277 | `--expected_obs_dim` 필수, v6 action 이름 수정 |
| `collect_demos.py` | 1073 | `--skill` 분기, gripper binary, **Skill2/3EnvWithCam 추가** |
| `convert_hdf5_to_lerobot_v3.py` | 525 | `convert_to_vla_units()` passthrough |
| `record_teleop.py` | 766 | `--skill` 분기, v6 action format |
| `deploy_vla_action_bridge.py` | 217 | `--action_format` v6/legacy 분기 추가 |

### 2-3. 수정 금지 파일

- `lekiwi_robot_cfg.py` — 로봇 설정
- `spawn_manager.py` — 물체 스폰 관리
- `calibration_common.py` — 공통 캘리브레이션
- 모든 calibration/comparison 스크립트

---

## 3. 전체 소스코드

### 3-1. models.py (115줄)

공유 RL 모델: PolicyNet, ValueNet, CriticNet(AAC). Orthogonal initialization 적용.
`PolicyNet.net + mean_layer` 구조는 `train_bc.py`의 `BCPolicy`와 동일하여 BC→PPO warm-start가 가능.

```python
"""
LeKiwi Navigation — Shared RL model definitions.

PolicyNet/ValueNet은 train_lekiwi.py, collect_demos.py 등에서 공통 사용.
구조 변경 시 이 파일만 수정하면 모든 스크립트에 반영됨.

PolicyNet.net + mean_layer 구조는 train_bc.py의 BCPolicy와 동일해야
BC → PPO warm-start가 정상 동작함.
"""
from __future__ import annotations

import math

import torch
import torch.nn as nn

from skrl.models.torch import DeterministicMixin, GaussianMixin, Model


def _ortho_init(module: nn.Module, gain: float = math.sqrt(2)):
    """Orthogonal initialization (PPO 37 implementation details)."""
    if isinstance(module, nn.Linear):
        nn.init.orthogonal_(module.weight, gain=gain)
        if module.bias is not None:
            nn.init.constant_(module.bias, 0.0)


class PolicyNet(GaussianMixin, Model):
    """Gaussian Policy (Actor).

    구조가 train_bc.py의 BCPolicy와 동일 (net + mean_layer).
    """

    def __init__(self, observation_space, action_space, device, **kwargs):
        Model.__init__(self, observation_space, action_space, device, **kwargs)
        GaussianMixin.__init__(
            self,
            clip_actions=True,
            clip_log_std=True,
            min_log_std=-5.0,
            max_log_std=2.0,
        )

        obs_dim = observation_space.shape[0]
        act_dim = action_space.shape[0]

        self.net = nn.Sequential(
            nn.Linear(obs_dim, 256),
            nn.ELU(),
            nn.Linear(256, 128),
            nn.ELU(),
            nn.Linear(128, 64),
            nn.ELU(),
        )
        self.mean_layer = nn.Linear(64, act_dim)
        self.log_std_parameter = nn.Parameter(torch.full((act_dim,), -1.0))

        # Orthogonal init: hidden=sqrt(2), policy output=0.01
        self.net.apply(lambda m: _ortho_init(m, gain=math.sqrt(2)))
        _ortho_init(self.mean_layer, gain=0.01)

    def compute(self, inputs, role):
        x = self.net(inputs["states"])
        return self.mean_layer(x), self.log_std_parameter, {}


class ValueNet(DeterministicMixin, Model):
    """Value Function (Critic)."""

    def __init__(self, observation_space, action_space, device, **kwargs):
        Model.__init__(self, observation_space, action_space, device, **kwargs)
        DeterministicMixin.__init__(self, clip_actions=False)

        obs_dim = observation_space.shape[0]

        self.net = nn.Sequential(
            nn.Linear(obs_dim, 256),
            nn.ELU(),
            nn.Linear(256, 128),
            nn.ELU(),
            nn.Linear(128, 64),
            nn.ELU(),
            nn.Linear(64, 1),
        )
        # Orthogonal init: hidden=sqrt(2), value output=1.0
        for m in list(self.net.children())[:-1]:
            if isinstance(m, nn.Linear):
                _ortho_init(m, gain=math.sqrt(2))
        _ortho_init(self.net[-1], gain=1.0)  # output layer

    def compute(self, inputs, role):
        return self.net(inputs["states"]), {}


class CriticNet(DeterministicMixin, Model):
    """Asymmetric Critic — Actor보다 넓은 observation을 받는다."""

    def __init__(self, observation_space, action_space, device,
                 critic_obs_dim=None, **kwargs):
        Model.__init__(self, observation_space, action_space, device, **kwargs)
        DeterministicMixin.__init__(self, clip_actions=False)

        obs_dim = critic_obs_dim if critic_obs_dim is not None else observation_space.shape[0]

        self.net = nn.Sequential(
            nn.Linear(obs_dim, 256),
            nn.ELU(),
            nn.Linear(256, 128),
            nn.ELU(),
            nn.Linear(128, 64),
            nn.ELU(),
            nn.Linear(64, 1),
        )
        # Orthogonal init: hidden=sqrt(2), value output=1.0
        for m in list(self.net.children())[:-1]:
            if isinstance(m, nn.Linear):
                _ortho_init(m, gain=math.sqrt(2))
        _ortho_init(self.net[-1], gain=1.0)  # output layer

    def compute(self, inputs, role):
        return self.net(inputs["states"]), {}
```

---

### 3-2. lekiwi_skill2_env.py (1647줄)

Skill-2 ApproachAndGrasp 환경. FSM 없이 단일 목표(approach→grasp→lift) 수행.
**Audit Fix**: curriculum 초기화 config 읽기 (H2), `_critic_obs` 저장 (C2).

**Observation 구조:**
- Actor 30D: `[arm5, grip1, base_body_vel3, lin_vel3, ang_vel3, arm_vel6, obj_rel3, contact2, bbox3, cat1]`
- Critic 37D (AAC): `Actor 30D + obj_bbox3 + mass1 + obj_dist1 + heading1 + vel_toward1`

**주요 섹션:**
- `Skill2EnvCfg` (62-165): 환경 설정, break_force=30N, curriculum
- `__init__` (167-319): 인덱스, 버퍼, catalog 초기화
- `_setup_scene` (325-456): 로봇/물체/센서 등록, gripper body `"Moving_Jaw_08d_v1"`
- Calibration/Dynamics (461-782): JSON 기반 dynamics 적용
- Grasp attach (787-937): Fixed joint 기반 물리 grasp
- DR (942-1117): wheel/arm stiffness-damping, break_force DR 15~45N
- `_read_base_body_vel` (1185-1195): `root_lin_vel_b` / `root_ang_vel_b` 직접 읽기
- `_compute_metrics` (1201-1243): object 상대 위치/거리/속도
- `_update_grasp_state` (1249-1314): contact 기반 grasp, break 감지 (dist>0.15m)
- `_apply_action` (1320-1361): v6 순서 `[arm5, grip1, base3]`
- `_get_observations` (1367-1415): 30D actor + 37D critic
- `_get_rewards` (1421-1476): approach progress + grasp bonus + lift bonus
- `_get_dones` (1482-1519): curriculum 업데이트
- `_reset_idx` (1525-1638): object 랜덤 배치, curriculum dist 적용

> 파일이 매우 긺 (1638줄). 전체 코드는 `lekiwi_skill2_env.py` 파일을 직접 참조.

주요 코드 발췌 — Observation (30D Actor + 37D Critic):

```python
def _get_observations(self) -> dict:
    metrics = self._cached_metrics if self._cached_metrics is not None else self._compute_metrics()
    base_body_vel = self._read_base_body_vel()  # (N, 3) — vx, vy, wz

    arm_pos = metrics["arm_pos"]
    arm_vel = metrics["arm_vel"]
    lin_vel = metrics["lin_vel_b"]
    ang_vel = metrics["ang_vel_b"]

    rel_object = metrics["object_pos_b"]

    contact_force = self._contact_force_per_env()
    contact_binary = (contact_force > float(self.cfg.grasp_contact_threshold)).float()
    contact_lr = torch.stack([contact_binary, contact_binary], dim=-1)

    bbox_norm = self.object_bbox / float(self._bbox_norm_scale)
    cat_denom = max(int(self.cfg.num_object_categories) - 1, 1)
    cat_norm = (self.object_category_id / float(cat_denom)).unsqueeze(-1)

    # Observation noise (sim2real: 센서 노이즈 시뮬레이션, 학습 시에만)
    if bool(self.cfg.enable_domain_randomization):
        jp_noise = float(self.cfg.dr_obs_noise_joint_pos)
        bv_noise = float(self.cfg.dr_obs_noise_base_vel)
        or_noise = float(self.cfg.dr_obs_noise_object_rel)
        if jp_noise > 0:
            arm_pos = arm_pos + torch.randn_like(arm_pos) * jp_noise
        if bv_noise > 0:
            base_body_vel = base_body_vel + torch.randn_like(base_body_vel) * bv_noise
            lin_vel = lin_vel + torch.randn_like(lin_vel) * bv_noise
            ang_vel = ang_vel + torch.randn_like(ang_vel) * bv_noise
        if or_noise > 0:
            rel_object = rel_object + torch.randn_like(rel_object) * or_noise

    # Actor Observation (30D)
    actor_obs = torch.cat([
        arm_pos[:, :5],             # [0:5]   arm joint pos 5D
        arm_pos[:, 5:6],            # [5:6]   gripper pos 1D
        base_body_vel,              # [6:9]   base body velocity 3D (m/s, rad/s)
        lin_vel,                    # [9:12]  base linear vel 3D
        ang_vel,                    # [12:15] base angular vel 3D
        arm_vel,                    # [15:21] arm+grip joint vel 6D
        rel_object,                 # [21:24] object relative pos 3D
        contact_lr,                 # [24:26] contact L/R 2D
        bbox_norm,                  # [26:29] object bbox 3D
        cat_norm,                   # [29:30] object category 1D
    ], dim=-1)  # 30D

    self._cached_metrics = None

    # Critic Observation (37D, AAC)
    obj_mass_per_env = self._catalog_mass[
        self.active_object_idx.clamp(max=len(self._catalog_mass) - 1)
    ].unsqueeze(-1)
    critic_extra = torch.cat([
        self.object_bbox,                            # 3D (원본, 비정규화)
        obj_mass_per_env,                            # 1D
        metrics["object_dist"].unsqueeze(-1),        # 1D
        metrics["heading_object"].unsqueeze(-1),     # 1D
        metrics["vel_toward_object"].unsqueeze(-1),  # 1D
    ], dim=-1)  # 7D
    critic_obs = torch.cat([actor_obs, critic_extra], dim=-1)  # 37D

    # AAC wrapper가 state()로 critic obs를 노출할 수 있도록 저장
    self._critic_obs = critic_obs

    return {"policy": actor_obs, "critic": critic_obs}
```

주요 코드 발췌 — Action 순서 (v6):

```python
def _apply_action(self):
    # 새 순서: [arm5, grip1, base3]
    arm_grip_action = self.actions[:, 0:6]

    base_vx = self.actions[:, 6] * self.cfg.max_lin_vel
    base_vy = self.actions[:, 7] * self.cfg.max_lin_vel
    base_wz = self.actions[:, 8] * self.cfg.max_ang_vel

    # Base -> Kiwi IK -> Wheel
    body_cmd = torch.stack([base_vx, base_vy, base_wz], dim=-1)
    wheel_radps = body_cmd @ self.kiwi_M.T / self.wheel_radius

    vel_target = torch.zeros(self.num_envs, self.robot.num_joints, device=self.device)
    vel_target[:, self.wheel_idx] = wheel_radps
    self.robot.set_joint_velocity_target(vel_target)

    # Arm -> Position Target
    if self.cfg.arm_action_to_limits:
        if self._arm_action_limits_override is not None:
            arm_limits = self._arm_action_limits_override
        else:
            arm_limits = self.robot.data.soft_joint_pos_limits[:, self.arm_idx]
        arm_lo = arm_limits[..., 0]
        arm_hi = arm_limits[..., 1]
        finite = torch.isfinite(arm_lo) & torch.isfinite(arm_hi) & ((arm_hi - arm_lo) > 1e-6)

        center = 0.5 * (arm_lo + arm_hi)
        half = 0.5 * (arm_hi - arm_lo)
        mapped = center + arm_grip_action * half
        fallback = arm_grip_action * self.cfg.arm_action_scale
        arm_targets = torch.where(finite, mapped, fallback)
        arm_targets = torch.where(finite, torch.clamp(arm_targets, arm_lo, arm_hi), arm_targets)
    else:
        arm_targets = arm_grip_action * self.cfg.arm_action_scale

    pos_target = torch.zeros(self.num_envs, self.robot.num_joints, device=self.device)
    pos_target[:, self.arm_idx] = arm_targets
    self.robot.set_joint_position_target(pos_target)
```

주요 코드 발췌 — Body velocity 직접 읽기:

```python
def _read_base_body_vel(self):
    """Body-frame velocity 직접 읽기 (3D: vx, vy, wz).

    Isaac Sim의 root_lin_vel_b, root_ang_vel_b에서 직접 추출.
    이전 설계의 pose delta → body-frame 변환 계산이 불필요하다.
    단위: m/s (vx, vy), rad/s (wz) — 실제 로봇과 동일.
    """
    vx_body = self.robot.data.root_lin_vel_b[:, 0:1]   # x.vel (m/s)
    vy_body = self.robot.data.root_lin_vel_b[:, 1:2]   # y.vel (m/s)
    wz_body = self.robot.data.root_ang_vel_b[:, 2:3]   # theta.vel (rad/s)
    return torch.cat([vx_body, vy_body, wz_body], dim=-1)  # (N, 3)
```

주요 코드 발췌 — Grasp break 감지:

```python
# Grasp break 감지 (fixed joint 파손 시 drop 판정)
if self.object_grasped.any() and self._physics_grasp:
    grip_pos_w = self.robot.data.body_pos_w[:, self._gripper_body_idx]
    obj_delta = self.object_pos_w - grip_pos_w
    grip_obj_dist = torch.norm(obj_delta, dim=-1)
    drop_detected = self.object_grasped & (grip_obj_dist > float(self.cfg.grasp_drop_detect_dist))
    if drop_detected.any():
        self.object_grasped[drop_detected] = False
        self.just_dropped[drop_detected] = True
        drop_ids = drop_detected.nonzero(as_tuple=False).squeeze(-1)
        if self._grasp_attach_mode == "fixed_joint":
            self._disable_grasp_fixed_joint_for_envs(drop_ids)
```

주요 코드 발췌 — Curriculum (H2 수정: config에서 초기값 읽기):

```python
# __init__ 내부 — curriculum 초기화 (Audit H2 수정):
if (hasattr(self.cfg, 'curriculum_current_max_dist')
        and float(self.cfg.curriculum_current_max_dist) > float(self.cfg.object_dist_min)):
    self._curriculum_dist = min(
        float(self.cfg.curriculum_current_max_dist),
        float(self.cfg.object_dist_max),
    )
else:
    self._curriculum_dist = float(self.cfg.object_dist_min)

# Curriculum 업데이트
if self.task_success.any() or time_out.any():
    done_mask = self.task_success | time_out | terminated
    if done_mask.any():
        batch_success = self.task_success[done_mask].float().mean().item()
        self._curriculum_success_window[self._curriculum_idx % 100] = batch_success
        self._curriculum_idx += 1
        if self._curriculum_idx >= 100:
            avg = self._curriculum_success_window.mean().item()
            if avg > self.cfg.curriculum_success_threshold:
                old = self._curriculum_dist
                self._curriculum_dist = min(
                    self._curriculum_dist + self.cfg.curriculum_dist_increment,
                    self.cfg.object_dist_max,
                )
                if self._curriculum_dist != old:
                    print(f"  [Curriculum] dist: {old:.2f} -> {self._curriculum_dist:.2f}")
                    # 이전 난이도의 성공률이 남아 조기 재상승하는 것을 방지
                    self._curriculum_success_window.zero_()
                    self._curriculum_idx = 0
```

주요 코드 발췌 — Gripper body 이름:

```python
# _setup_scene 내부:
# Grasp break 감지를 위한 gripper body index
_gripper_body_names = ["Moving_Jaw_08d_v1"]
try:
    body_ids, _ = self.robot.find_bodies(_gripper_body_names)
    self._gripper_body_idx = body_ids[0]
except (IndexError, RuntimeError):
    self._gripper_body_idx = 0
```

주요 코드 발췌 — Skill2EnvCfg (주요 파라미터):

```python
@configclass
class Skill2EnvCfg(DirectRLEnvCfg):
    observation_space: int = 30     # Actor
    action_space: int = 9
    state_space: int = 37           # Critic (AAC)

    max_lin_vel: float = 0.5
    max_ang_vel: float = 3.0
    arm_action_scale: float = 1.5
    arm_action_to_limits: bool = True

    object_dist_min: float = 0.5
    object_dist_max: float = 2.5
    curriculum_success_threshold: float = 0.7
    curriculum_dist_increment: float = 0.25
    curriculum_current_max_dist: float = 0.5

    grasp_joint_break_force: float = 30.0    # mass*g*10
    grasp_joint_break_torque: float = 30.0
    grasp_drop_detect_dist: float = 0.15

    # Grasp DR
    dr_grasp_break_force_range: tuple[float, float] = (15.0, 45.0)
    dr_grasp_break_torque_range: tuple[float, float] = (15.0, 45.0)
```

---

### 3-3. lekiwi_skill3_env.py (494줄)

Skill-3 CarryAndPlace 환경. Skill2Env를 상속하여 "잡은 물체를 home까지 운반 후 놓기" 수행.
**Audit Fix**: Intentional place 메커니즘 추가 (C1), `_critic_obs` 저장 (C2).
**Audit Fix 2**: `_get_dones()`에서 place_success로 task_success 오버라이드 (CRITICAL-2), `_reset_from_handoff()` 배치 텐서 생성 (NEW-5).

```python
"""
LeKiwi Skill-3 — CarryAndPlace Isaac Lab DirectRLEnv.

3-Skill 파이프라인의 세 번째 스킬: 잡은 물체를 들고 home까지 운반 후 놓기(Place).
Handoff Buffer에서 초기 상태(물체가 이미 잡힌 상태)를 로드하여 시작.

Observation (Actor 29D):
  [0:5]   arm joint pos (5)
  [5:6]   gripper pos (1)
  [6:9]   base body velocity (vx, vy, wz) (3)
  [9:12]  base linear vel body (3)
  [12:15] base angular vel body (3)
  [15:21] arm+grip joint vel (6)
  [21:24] home relative pos body (3)
  [24:25] grip force (1)
  [25:28] object bbox normalized (3)
  [28:29] object category normalized (1)

Observation (Critic 36D, AAC):
  Actor 29D + object_bbox(3) + mass(1) + gripper_rel_pos(3)

Action (9D — lekiwi_v6 순서):
  [0:5]   arm joint position target
  [5]     gripper position target
  [6:8]   base linear velocity (vx, vy)
  [8]     base angular velocity (wz)
"""
from __future__ import annotations

import math
import pickle
from typing import Dict

import torch
from isaaclab.utils import configclass
from isaaclab.utils.math import quat_apply_inverse

from lekiwi_skill2_env import Skill2Env, Skill2EnvCfg


@configclass
class Skill3EnvCfg(Skill2EnvCfg):
    """CarryAndPlace 환경 설정."""

    observation_space: int = 29
    state_space: int = 36

    # Task
    return_thresh: float = 0.30
    place_dist_thresh: float = 0.05
    place_gripper_threshold: float = 0.3  # gripper pos > 이 값이면 open 판정

    # Handoff
    handoff_buffer_path: str = ""

    # Reward (CarryAndPlace 전용)
    rew_carry_progress_weight: float = 3.0
    rew_carry_heading_weight: float = 0.2
    rew_hold_bonus: float = 0.1
    rew_place_success_bonus: float = 20.0
    rew_drop_penalty: float = -10.0

    # Handoff noise (per-load: 같은 entry를 여러 번 로드해도 다른 state)
    handoff_arm_noise_std: float = 0.02       # ~1deg joint noise
    handoff_base_pos_noise_std: float = 0.01  # 1cm position noise
    handoff_base_yaw_noise_std: float = 0.02  # ~1deg heading noise

    # Curriculum 제거 (Skill-3는 불필요)
    curriculum_success_threshold: float = 1.0


class Skill3Env(Skill2Env):
    """LeKiwi Skill-3: CarryAndPlace RL 환경."""

    cfg: Skill3EnvCfg

    def __init__(self, cfg: Skill3EnvCfg, render_mode: str | None = None, **kwargs):
        # Handoff buffer 로드
        self.handoff_buffer = None
        if cfg.handoff_buffer_path:
            import os
            buf_path = os.path.expanduser(cfg.handoff_buffer_path)
            if os.path.isfile(buf_path):
                with open(buf_path, "rb") as f:
                    self.handoff_buffer = pickle.load(f)
                print(f"  [Skill3Env] Loaded handoff buffer: {len(self.handoff_buffer)} entries from {buf_path}")
            else:
                print(f"  [WARN] Handoff buffer not found: {buf_path}")

        super().__init__(cfg, render_mode, **kwargs)

        # Skill-3 추가 버퍼
        self.prev_home_dist = torch.zeros(self.num_envs, device=self.device)
        self.intentional_placed = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
        # Handoff buffer에서 읽은 object orientation (identity default)
        self._handoff_object_ori = torch.zeros(self.num_envs, 4, device=self.device)
        self._handoff_object_ori[:, 0] = 1.0  # qw=1 identity

        print(f"  [Skill3Env] obs={self.cfg.observation_space} act={self.cfg.action_space} critic={self.cfg.state_space}")

    # ═══════════════════════════════════════════════════════════════════
    #  Metrics — home 관련 추가
    # ═══════════════════════════════════════════════════════════════════

    def _compute_metrics(self) -> Dict[str, torch.Tensor]:
        metrics = super()._compute_metrics()

        root_pos_w = self.robot.data.root_pos_w
        root_quat_w = self.robot.data.root_quat_w

        home_delta_w = self.home_pos_w - root_pos_w
        home_pos_b = quat_apply_inverse(root_quat_w, home_delta_w)
        home_dist = torch.norm(home_pos_b[:, :2], dim=-1)
        heading_home = torch.atan2(home_pos_b[:, 1], home_pos_b[:, 0])

        metrics["home_pos_b"] = home_pos_b
        metrics["home_dist"] = home_dist
        metrics["heading_home"] = heading_home
        return metrics

    # ═══════════════════════════════════════════════════════════════════
    #  Grasp state — intentional place 추가 (Audit C1 수정)
    # ═══════════════════════════════════════════════════════════════════

    def _update_grasp_state(self, metrics: Dict[str, torch.Tensor]):
        """Skill-3 grasp state: 부모 로직 + intentional place 메커니즘.

        Home 근처에서 gripper를 열면 FixedJoint를 해제하여 물체를 의도적으로
        내려놓는다. 이때 just_dropped=False를 유지하여 place_success 조건이
        충족될 수 있도록 한다 (accidental drop과 구분).
        """
        self.intentional_placed[:] = False

        # 부모(Skill2) grasp 로직 실행 (can_grasp, lift, drop detection)
        super()._update_grasp_state(metrics)

        # Intentional place: 물체를 잡은 상태 + gripper open + home 근처
        if self.object_grasped.any():
            gripper_pos = self.robot.data.joint_pos[:, self.gripper_idx]
            gripper_open = gripper_pos > float(self.cfg.place_gripper_threshold)
            home_dist = metrics.get("home_dist", None)
            if home_dist is None:
                home_delta_w = self.home_pos_w - self.robot.data.root_pos_w
                home_pos_b = quat_apply_inverse(self.robot.data.root_quat_w, home_delta_w)
                home_dist = torch.norm(home_pos_b[:, :2], dim=-1)
            near_home = home_dist < self.cfg.return_thresh

            intentional_place = self.object_grasped & gripper_open & near_home
            if intentional_place.any():
                place_ids = intentional_place.nonzero(as_tuple=False).squeeze(-1)
                if self._physics_grasp and self._grasp_attach_mode == "fixed_joint":
                    self._disable_grasp_fixed_joint_for_envs(place_ids)
                self.object_grasped[intentional_place] = False
                self.intentional_placed[intentional_place] = True
                # NOTE: just_dropped는 False 유지 → place_success 달성 가능

    # ═══════════════════════════════════════════════════════════════════
    #  Observations — 29D Actor
    # ═══════════════════════════════════════════════════════════════════

    def _get_observations(self) -> dict:
        metrics = self._cached_metrics if self._cached_metrics is not None else self._compute_metrics()
        base_body_vel = self._read_base_body_vel()  # (N, 3) — vx, vy, wz

        arm_pos = metrics["arm_pos"]
        arm_vel = metrics["arm_vel"]
        lin_vel = metrics["lin_vel_b"]
        ang_vel = metrics["ang_vel_b"]

        # home_rel: body-frame 상대 벡터 3D
        home_delta_w = self.home_pos_w - self.robot.data.root_pos_w
        home_rel = quat_apply_inverse(self.robot.data.root_quat_w, home_delta_w)

        # grip_force: 스칼라 1D
        contact_force = self._contact_force_per_env()
        grip_force = contact_force.unsqueeze(-1)

        # Observation noise (sim2real: 센서 노이즈 시뮬레이션, 학습 시에만)
        if bool(self.cfg.enable_domain_randomization):
            jp_noise = float(self.cfg.dr_obs_noise_joint_pos)
            bv_noise = float(self.cfg.dr_obs_noise_base_vel)
            or_noise = float(self.cfg.dr_obs_noise_object_rel)
            if jp_noise > 0:
                arm_pos = arm_pos + torch.randn_like(arm_pos) * jp_noise
            if bv_noise > 0:
                base_body_vel = base_body_vel + torch.randn_like(base_body_vel) * bv_noise
                lin_vel = lin_vel + torch.randn_like(lin_vel) * bv_noise
                ang_vel = ang_vel + torch.randn_like(ang_vel) * bv_noise
            if or_noise > 0:
                home_rel = home_rel + torch.randn_like(home_rel) * or_noise

        # BBox / Category
        bbox_norm = self.object_bbox / float(self._bbox_norm_scale)
        cat_denom = max(int(self.cfg.num_object_categories) - 1, 1)
        cat_norm = (self.object_category_id / float(cat_denom)).unsqueeze(-1)

        actor_obs = torch.cat([
            arm_pos[:, :5],             # [0:5]   arm 5D
            arm_pos[:, 5:6],            # [5:6]   gripper 1D
            base_body_vel,              # [6:9]   base body velocity 3D (m/s, rad/s)
            lin_vel,                    # [9:12]  base_lin_vel 3D
            ang_vel,                    # [12:15] base_ang_vel 3D
            arm_vel,                    # [15:21] arm+grip vel 6D
            home_rel,                   # [21:24] home relative 3D
            grip_force,                 # [24:25] grip force 1D
            bbox_norm,                  # [25:28] bbox 3D
            cat_norm,                   # [28:29] category 1D
        ], dim=-1)  # 29D

        self._cached_metrics = None

        # Critic Observation (36D, AAC)
        # Actor 29D + obj_dimensions(3D) + obj_mass(1D) + gripper_rel_pos(3D) = 36D
        obj_mass_per_env = self._catalog_mass[
            self.active_object_idx.clamp(max=len(self._catalog_mass) - 1)
        ].unsqueeze(-1)
        # gripper_rel_pos: object position relative to gripper body (world-frame)
        grip_pos_w = self.robot.data.body_pos_w[:, self._gripper_body_idx]
        gripper_rel_pos = self.object_pos_w - grip_pos_w  # (N, 3)
        critic_extra = torch.cat([
            self.object_bbox,                            # 3D (obj_dimensions)
            obj_mass_per_env,                            # 1D
            gripper_rel_pos,                             # 3D (gripper-to-object)
        ], dim=-1)  # 7D
        critic_obs = torch.cat([actor_obs, critic_extra], dim=-1)  # 36D

        # AAC wrapper가 state()로 critic obs를 노출할 수 있도록 저장
        self._critic_obs = critic_obs

        return {"policy": actor_obs, "critic": critic_obs}

    # ═══════════════════════════════════════════════════════════════════
    #  Rewards — CarryAndPlace
    # ═══════════════════════════════════════════════════════════════════

    def _get_rewards(self) -> torch.Tensor:
        # _cached_metrics 사용 — _get_dones()에서 이미 _update_grasp_state() 실행됨
        metrics = self._cached_metrics
        if metrics is None:
            metrics = self._compute_metrics()
            self._update_grasp_state(metrics)

        reward = torch.full((self.num_envs,), self.cfg.rew_time_penalty, device=self.device)

        # Action smoothness penalty (sim2real: 실기에서 부드러운 동작 유도)
        action_delta = self.actions - self.prev_actions
        reward += self.cfg.rew_action_smoothness_weight * (action_delta ** 2).sum(dim=-1)

        # Carry: home까지 거리 줄이기
        home_progress = torch.clamp(self.prev_home_dist - metrics["home_dist"], -0.2, 0.2)
        reward += self.cfg.rew_carry_progress_weight * home_progress
        reward += self.cfg.rew_carry_heading_weight * torch.cos(metrics["heading_home"])

        # Hold bonus — 물체를 아직 들고 있을 때만
        reward += self.cfg.rew_hold_bonus * self.object_grasped.float()

        # Drop penalty — grasp break 감지 시 즉시 발동
        reward += self.just_dropped.float() * self.cfg.rew_drop_penalty

        # Place 성공: home 근처에서 의도적으로 놓음 (drop이 아닌 경우)
        place_dist = torch.norm(self.object_pos_w[:, :2] - self.home_pos_w[:, :2], dim=-1)
        near_home = place_dist < self.cfg.return_thresh
        place_success = (~self.object_grasped) & near_home & (~self.just_dropped)
        reward += place_success.float() * self.cfg.rew_place_success_bonus
        self.task_success = place_success

        # Effort
        reward += self.cfg.rew_effort_weight * (self.actions[:, 6:9] ** 2).sum(dim=-1)
        reward += self.cfg.rew_arm_move_weight * (metrics["arm_vel"] ** 2).sum(dim=-1)

        self.prev_home_dist[:] = metrics["home_dist"]
        self.episode_reward_sum += reward
        return reward

    # ═══════════════════════════════════════════════════════════════════
    #  Dones
    # ═══════════════════════════════════════════════════════════════════

    def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
        metrics = self._compute_metrics()
        self._update_grasp_state(metrics)  # drop 감지 포함
        self._cached_metrics = metrics

        # ── CRITICAL-2 Fix: Override parent's lift-based task_success ──
        # Parent sets task_success=True for lifted objects, but Skill-3
        # starts with already-lifted objects from handoff buffer.
        # Use place_success (intentional place near home) instead.
        place_dist = torch.norm(
            self.object_pos_w[:, :2] - self.home_pos_w[:, :2], dim=-1
        )
        near_home = place_dist < self.cfg.return_thresh
        place_success = (~self.object_grasped) & near_home & (~self.just_dropped)
        self.task_success = place_success

        root_pos = metrics["root_pos_w"]
        out_of_bounds = torch.norm(
            root_pos[:, :2] - self.home_pos_w[:, :2], dim=-1
        ) > self.cfg.max_dist_from_origin
        env_z = self.scene.env_origins[:, 2] if hasattr(self.scene, "env_origins") else 0.0
        fell = ((root_pos[:, 2] - env_z) < 0.01) | ((root_pos[:, 2] - env_z) > 0.5)

        # Skill-3 핵심: drop → terminated (에피소드 즉시 종료)
        dropped = self.just_dropped
        terminated = out_of_bounds | fell | dropped

        time_out = self.episode_length_buf >= (self.max_episode_length - 1)
        truncated = self.task_success | time_out

        # Logging
        self.extras["task_success_rate"] = self.task_success.float().mean()
        self.extras["drop_rate"] = dropped.float().mean()

        return terminated, truncated

    # ═══════════════════════════════════════════════════════════════════
    #  Reset — Handoff Buffer 기반
    # ═══════════════════════════════════════════════════════════════════

    def _reset_idx(self, env_ids: torch.Tensor):
        # DirectRLEnv._reset_idx (Skill2Env._reset_idx를 건너뜀)
        DirectRLEnv_reset = super(Skill2Env, self)._reset_idx
        DirectRLEnv_reset(env_ids)
        num = len(env_ids)
        if num == 0:
            return

        if self._physics_grasp and self._grasp_attach_mode == "fixed_joint":
            self._disable_grasp_fixed_joint_for_envs(env_ids)

        if self.handoff_buffer is not None and len(self.handoff_buffer) > 0:
            self._reset_from_handoff(env_ids, num)
        else:
            # Fallback: Skill-2 방식으로 리셋 (물체 잡힌 상태로 시작)
            self._reset_fallback(env_ids, num)

    def _reset_from_handoff(self, env_ids: torch.Tensor, num: int):
        """Handoff Buffer에서 랜덤 샘플하여 리셋."""
        buf_size = len(self.handoff_buffer)
        indices = torch.randint(0, buf_size, (num,))

        # Batch: collect all entries first to avoid per-loop tensor creation
        entries = [self.handoff_buffer[indices[i].item()] for i in range(num)]

        root_states = self.robot.data.default_root_state[env_ids].clone()
        joint_positions = self.robot.data.default_joint_pos[env_ids].clone()

        # Batch tensor construction (CPU → GPU once)
        base_pos = torch.tensor([e["base_pos"] for e in entries], device=self.device, dtype=torch.float32)
        base_ori = torch.tensor([e["base_ori"] for e in entries], device=self.device, dtype=torch.float32)
        obj_pos = torch.tensor([e["object_pos"] for e in entries], device=self.device, dtype=torch.float32)
        obj_ori = torch.tensor(
            [e.get("object_ori", [1.0, 0.0, 0.0, 0.0]) for e in entries],
            device=self.device, dtype=torch.float32,
        )
        arm_joints = torch.tensor([e["arm_joints"] for e in entries], device=self.device, dtype=torch.float32)
        grip_states = torch.tensor([e["gripper_state"] for e in entries], device=self.device, dtype=torch.float32)
        obj_type_indices = torch.tensor([int(e["object_type_idx"]) for e in entries], device=self.device, dtype=torch.long)

        # 상대 좌표 → 절대 좌표 변환 (destination env의 origin 기준)
        env_origins = self.scene.env_origins[env_ids]  # (num, 3)
        base_pos = base_pos + env_origins
        obj_pos = obj_pos + env_origins

        # Per-load noise: 같은 handoff entry라도 매번 다른 state로 시작
        if self.cfg.handoff_arm_noise_std > 0:
            arm_joints = arm_joints + torch.randn_like(arm_joints) * self.cfg.handoff_arm_noise_std
            if self.cfg.arm_action_to_limits:
                arm_lo = self.robot.data.soft_joint_pos_limits[0, self.arm_idx[:5], 0]
                arm_hi = self.robot.data.soft_joint_pos_limits[0, self.arm_idx[:5], 1]
                arm_joints = torch.clamp(arm_joints, arm_lo, arm_hi)

        if self.cfg.handoff_base_pos_noise_std > 0:
            base_pos[:, :2] = base_pos[:, :2] + torch.randn(num, 2, device=self.device) * self.cfg.handoff_base_pos_noise_std

        if self.cfg.handoff_base_yaw_noise_std > 0:
            dyaw = torch.randn(num, device=self.device) * self.cfg.handoff_base_yaw_noise_std
            half = dyaw * 0.5
            dq = torch.zeros(num, 4, device=self.device)
            dq[:, 0] = torch.cos(half)   # w
            dq[:, 3] = torch.sin(half)   # z (yaw only)
            base_ori = quat_mul(dq, base_ori)

        # Robot root state
        root_states[:, 0:3] = base_pos
        root_states[:, 3:7] = base_ori

        # Arm+gripper joints (batched)
        for j in range(5):
            joint_positions[:, self.arm_idx[j]] = arm_joints[:, j]
        joint_positions[:, self.arm_idx[5]] = grip_states

        # Object position + orientation
        self.object_pos_w[env_ids] = obj_pos
        self._handoff_object_ori[env_ids] = obj_ori
        self.active_object_idx[env_ids] = obj_type_indices

        # Single-object sim write (multi_object=False일 때 sim에 실제 pose 반영)
        if not self._multi_object and self.object_rigid is not None:
            pose = self.object_rigid.data.default_root_state[env_ids, :7].clone()
            pose[:, :3] = obj_pos
            pose[:, 3:7] = obj_ori
            self.object_rigid.write_root_pose_to_sim(pose, env_ids)
            # 잔여 속도 제거 (이전 에피소드의 물체 움직임 방지)
            zero_vel = torch.zeros(num, 6, dtype=torch.float32, device=self.device)
            self.object_rigid.write_root_velocity_to_sim(zero_vel, env_ids)

        if self._multi_object:
            clamped_idx = obj_type_indices.clamp(max=len(self._catalog_bbox) - 1)
            self.object_bbox[env_ids] = self._catalog_bbox[clamped_idx]
            self.object_category_id[env_ids] = self._catalog_category[clamped_idx.clamp(max=len(self._catalog_category) - 1)]

        self.robot.write_root_state_to_sim(root_states, env_ids)
        joint_vel = torch.zeros_like(joint_positions)
        self.robot.write_joint_state_to_sim(joint_positions, joint_vel, env_ids=env_ids)

        # home = destination env의 origin (env_origin XY + robot Z)
        self.home_pos_w[env_ids, 0:2] = env_origins[:, 0:2]
        self.home_pos_w[env_ids, 2] = root_states[:, 2]

        # Multi-object hide/show
        if self._multi_object and len(self.object_rigids) > 0:
            zero_vel = torch.zeros(num, 6, dtype=torch.float32, device=self.device)
            for rigid in self.object_rigids:
                hide_pose = rigid.data.default_root_state[env_ids, :7].clone()
                hide_pose[:, 2] = -10.0
                rigid.write_root_pose_to_sim(hide_pose, env_ids=env_ids)
                rigid.write_root_velocity_to_sim(zero_vel, env_ids=env_ids)

            for i in range(num):
                eid = env_ids[i]
                oi = int(self.active_object_idx[eid].item())
                if oi < len(self.object_rigids):
                    rigid = self.object_rigids[oi]
                    pose = rigid.data.default_root_state[eid:eid+1, :7].clone()
                    pose[0, :3] = self.object_pos_w[eid]
                    pose[0, 3:7] = self._handoff_object_ori[eid]
                    rigid.write_root_pose_to_sim(pose, torch.tensor([eid.item()], device=self.device))

        # 물체가 이미 잡힌 상태로 시작
        self.object_grasped[env_ids] = True
        # DR을 joint attach 전에 실행 → break_force DR이 적용된 joint 생성
        self._apply_domain_randomization(env_ids)
        self._attach_grasp_fixed_joint_for_envs(env_ids)

        self._finish_reset(env_ids, num)

    def _reset_fallback(self, env_ids: torch.Tensor, num: int):
        """Handoff buffer 없이 fallback: 물체 가까이 + 잡힌 상태."""
        default_root_state = self.robot.data.default_root_state[env_ids].clone()
        root_xy_std = float(self.cfg.dr_root_xy_noise_std) if bool(self.cfg.enable_domain_randomization) else 0.1
        default_root_state[:, 0:2] += torch.randn(num, 2, device=self.device) * root_xy_std

        random_yaw = torch.rand(num, device=self.device) * 2.0 * math.pi - math.pi
        half_yaw = random_yaw * 0.5
        default_root_state[:, 3] = torch.cos(half_yaw)
        default_root_state[:, 4] = 0.0
        default_root_state[:, 5] = 0.0
        default_root_state[:, 6] = torch.sin(half_yaw)
        self.robot.write_root_state_to_sim(default_root_state, env_ids)

        joint_pos = self.robot.data.default_joint_pos[env_ids].clone()
        joint_vel = torch.zeros_like(joint_pos)
        self.robot.write_joint_state_to_sim(joint_pos, joint_vel, env_ids=env_ids)

        self.home_pos_w[env_ids] = default_root_state[:, :3]

        # 물체를 잡힌 위치에 배치
        self.object_pos_w[env_ids, :2] = default_root_state[:, :2]
        self.object_pos_w[env_ids, 2] = default_root_state[:, 2] + float(self.cfg.grasp_attach_height)

        # Single-object sim write (multi_object=False일 때 sim에 실제 pose 반영)
        if not self._multi_object and self.object_rigid is not None:
            pose = self.object_rigid.data.default_root_state[env_ids, :7].clone()
            pose[:, :3] = self.object_pos_w[env_ids]
            self.object_rigid.write_root_pose_to_sim(pose, env_ids)

        self.object_grasped[env_ids] = True
        self._apply_domain_randomization(env_ids)
        self._attach_grasp_fixed_joint_for_envs(env_ids)

        self._finish_reset(env_ids, num)

    def _finish_reset(self, env_ids: torch.Tensor, num: int):
        """공통 리셋 후처리. DR은 caller가 attach 전에 실행해야 함."""
        self.task_success[env_ids] = False
        self.just_grasped[env_ids] = False
        self.just_dropped[env_ids] = False
        self.intentional_placed[env_ids] = False
        self.prev_home_dist[env_ids] = 10.0
        self.prev_object_dist[env_ids] = 10.0
        self.grasp_entry_step[env_ids] = 0
        self.episode_reward_sum[env_ids] = 0.0
        self.actions[env_ids] = 0.0
        self.prev_actions[env_ids] = 0.0
        if self._action_delay_buf is not None:
            self._action_delay_buf[:, env_ids] = 0.0
```

---

### 3-4. generate_handoff_buffer.py (176줄)

Skill-2 Expert를 실행하여 성공 에피소드의 종료 상태를 Handoff Buffer (pickle)로 저장.
Noise injection으로 VLA의 부정확한 Skill-2 출력을 모사.
**Audit Fix**: AAC 체크포인트 호환 (C2), 로그 milestone 수정 (L3).

```python
#!/usr/bin/env python3
"""
Skill-2 Expert 실행 -> 성공 에피소드 종료 상태를 Handoff Buffer로 저장.

Usage:
    python generate_handoff_buffer.py \
      --checkpoint logs/ppo_skill2/best_agent.pt \
      --num_entries 500 --num_envs 64 \
      --output handoff_buffer.pkl \
      --multi_object_json object_catalog.json \
      --gripper_contact_prim_path "..." \
      --dynamics_json calibration/tuned_dynamics.json \
      --arm_limit_json calibration/arm_limits_real2sim.json \
      --headless
"""
import argparse
import os
import pickle
import sys

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from isaaclab.app import AppLauncher

parser = argparse.ArgumentParser()
parser.add_argument("--checkpoint", type=str, required=True)
parser.add_argument("--num_entries", type=int, default=500)
parser.add_argument("--num_envs", type=int, default=64)
parser.add_argument("--output", type=str, default="handoff_buffer.pkl")
parser.add_argument("--noise_arm_std", type=float, default=0.05,
                    help="Arm joint noise std (rad). VLA의 부정확한 grasp 상태 모사")
parser.add_argument("--noise_obj_xy_std", type=float, default=0.02,
                    help="Object position noise std (m)")
parser.add_argument("--noise_base_xy_std", type=float, default=0.03,
                    help="Base position noise std (m)")
parser.add_argument("--noise_base_yaw_std", type=float, default=0.1,
                    help="Base orientation noise std (rad)")
parser.add_argument("--dynamics_json", type=str, default=None)
parser.add_argument("--calibration_json", type=str, default=None)
parser.add_argument("--arm_limit_json", type=str, default=None)
parser.add_argument("--multi_object_json", type=str, default="")
parser.add_argument("--gripper_contact_prim_path", type=str, default="")
AppLauncher.add_app_launcher_args(parser)
args = parser.parse_args()
launcher = AppLauncher(args)
sim_app = launcher.app

import numpy as np
import torch
from lekiwi_skill2_env import Skill2Env, Skill2EnvCfg
from models import PolicyNet, ValueNet, CriticNet
from skrl.agents.torch.ppo import PPO, PPO_DEFAULT_CONFIG
from skrl.envs.wrappers.torch import wrap_env
from skrl.memories.torch import RandomMemory
from skrl.resources.preprocessors.torch import RunningStandardScaler
from aac_wrapper import wrap_env_aac
from aac_ppo import AAC_PPO


def main():
    env_cfg = Skill2EnvCfg()
    env_cfg.scene.num_envs = args.num_envs
    if args.dynamics_json:
        env_cfg.dynamics_json = os.path.expanduser(args.dynamics_json)
    if args.calibration_json:
        env_cfg.calibration_json = os.path.expanduser(args.calibration_json)
    if args.arm_limit_json:
        env_cfg.arm_limit_json = os.path.expanduser(args.arm_limit_json)
    if args.multi_object_json:
        env_cfg.multi_object_json = os.path.expanduser(args.multi_object_json)
    env_cfg.gripper_contact_prim_path = args.gripper_contact_prim_path
    # Curriculum을 최대로 열어서 다양한 거리 커버
    env_cfg.object_dist_min = 0.5
    env_cfg.curriculum_current_max_dist = env_cfg.object_dist_max

    env = Skill2Env(cfg=env_cfg)
    wrapped = wrap_env_aac(env)
    device = wrapped.device

    critic_obs_dim = wrapped._aac_state_space.shape[0] if wrapped._aac_state_space is not None else None
    models = {
        "policy": PolicyNet(wrapped.observation_space, wrapped.action_space, device),
        "value": CriticNet(
            wrapped.observation_space, wrapped.action_space, device,
            critic_obs_dim=critic_obs_dim,
        ) if critic_obs_dim else ValueNet(wrapped.observation_space, wrapped.action_space, device),
    }
    memory = RandomMemory(memory_size=24, num_envs=args.num_envs, device=device)
    cfg_ppo = PPO_DEFAULT_CONFIG.copy()
    cfg_ppo["state_preprocessor"] = RunningStandardScaler
    cfg_ppo["state_preprocessor_kwargs"] = {"size": wrapped.observation_space, "device": device}
    cfg_ppo["value_preprocessor"] = RunningStandardScaler
    cfg_ppo["value_preprocessor_kwargs"] = {"size": 1, "device": device}
    if critic_obs_dim and wrapped._aac_state_space is not None:
        cfg_ppo["critic_state_preprocessor"] = RunningStandardScaler
        cfg_ppo["critic_state_preprocessor_kwargs"] = {"size": wrapped._aac_state_space, "device": device}

    if critic_obs_dim and wrapped._aac_state_space is not None:
        agent = AAC_PPO(models=models, memory=memory, cfg=cfg_ppo,
                        observation_space=wrapped.observation_space,
                        action_space=wrapped.action_space, device=device,
                        critic_observation_space=wrapped._aac_state_space)
    else:
        agent = PPO(models=models, memory=memory, cfg=cfg_ppo,
                    observation_space=wrapped.observation_space,
                    action_space=wrapped.action_space, device=device)
    agent.load(args.checkpoint)
    agent.set_running_mode("eval")

    entries = []
    obs, _ = env.reset()
    print(f"\n  Collecting {args.num_entries} handoff entries...")

    while len(entries) < args.num_entries:
        with torch.no_grad():
            action = agent.act({"states": obs["policy"]}, timestep=0, timesteps=1)[0]
        obs, _, terminated, truncated, _ = env.step(action)

        success = env.task_success
        if success.any():
            sids = success.nonzero(as_tuple=False).squeeze(-1)
            for sid in sids:
                i = sid.item()
                # Active object의 orientation 읽기
                oi = int(env.active_object_idx[i].item())
                if env._multi_object and oi < len(env.object_rigids):
                    obj_quat = env.object_rigids[oi].data.root_quat_w[i].cpu().tolist()
                elif env.object_rigid is not None:
                    obj_quat = env.object_rigid.data.root_quat_w[i].cpu().tolist()
                else:
                    obj_quat = [1.0, 0.0, 0.0, 0.0]  # identity

                # env_origin 기준 상대 좌표로 저장 (다른 env에 로드해도 정상 동작)
                origin = env.scene.env_origins[i]
                entry = {
                    "base_pos": (env.robot.data.root_pos_w[i] - origin).cpu().tolist(),
                    "base_ori": env.robot.data.root_quat_w[i].cpu().tolist(),
                    "arm_joints": env.robot.data.joint_pos[i, env.arm_idx[:5]].cpu().tolist(),
                    "gripper_state": env.robot.data.joint_pos[i, env.arm_idx[5]].item(),
                    "object_pos": (env.object_pos_w[i] - origin).cpu().tolist(),
                    "object_ori": obj_quat,
                    "object_type_idx": env.active_object_idx[i].item(),
                }

                # Noise injection — VLA의 부정확한 Skill-2 출력을 모사
                if args.noise_arm_std > 0:
                    entry["arm_joints"] = [
                        v + np.random.normal(0, args.noise_arm_std) for v in entry["arm_joints"]
                    ]
                if args.noise_obj_xy_std > 0:
                    entry["object_pos"][0] += np.random.normal(0, args.noise_obj_xy_std)
                    entry["object_pos"][1] += np.random.normal(0, args.noise_obj_xy_std)
                if args.noise_base_xy_std > 0:
                    entry["base_pos"][0] += np.random.normal(0, args.noise_base_xy_std)
                    entry["base_pos"][1] += np.random.normal(0, args.noise_base_xy_std)
                if args.noise_base_yaw_std > 0:
                    w, x, y, z = entry["base_ori"]
                    cur_yaw = 2.0 * np.arctan2(z, w)
                    new_yaw = cur_yaw + np.random.normal(0, args.noise_base_yaw_std)
                    entry["base_ori"] = [float(np.cos(new_yaw/2)), 0.0, 0.0, float(np.sin(new_yaw/2))]

                entries.append(entry)

        prev_milestone = (len(entries) - sids.numel()) // 50
        curr_milestone = len(entries) // 50
        if curr_milestone > prev_milestone and len(entries) > 0:
            print(f"    {len(entries)}/{args.num_entries}")

    entries = entries[:args.num_entries]
    with open(args.output, "wb") as f:
        pickle.dump(entries, f)
    print(f"\n  Saved {len(entries)} entries to {args.output}")

    env.close()
    sim_app.close()


if __name__ == "__main__":
    main()
```

---

### 3-5. train_lekiwi.py (526줄)

PPO 학습 스크립트. `--skill` 인자로 환경 분기, BC warm-start 지원.
**Audit Fix**: AAC 전면 통합 — `aac_wrapper` + `AAC_PPO` + `AACSequentialTrainer` (C2).

> 전체 코드: `train_lekiwi.py` 파일 직접 참조 (526줄).

**주요 기능:**
- `--skill` 분기: `approach_and_grasp` → Skill2Env, `carry_and_place` → Skill3Env, `legacy` → LeKiwiNavEnv
- `load_bc_into_policy()`: BC weight transfer (obs_dim mismatch 시 공통 부분만 복사)
- BC fine-tune 시 LR 스케일링 (기본 0.3 → 9e-5)
- **AAC 통합**: Skill-2/3에서 `wrap_env_aac()` + `CriticNet` + `AAC_PPO` + `AACSequentialTrainer` 사용

```python
# AAC import:
from aac_wrapper import wrap_env_aac
from aac_ppo import AAC_PPO
from aac_trainer import AACSequentialTrainer

# 핵심 분기 코드:
if args.skill == "approach_and_grasp":
    from lekiwi_skill2_env import Skill2Env, Skill2EnvCfg
    env_cfg = Skill2EnvCfg()
elif args.skill == "carry_and_place":
    from lekiwi_skill3_env import Skill3Env, Skill3EnvCfg
    if not args.handoff_buffer:
        raise ValueError("--handoff_buffer required for carry_and_place skill")
    env_cfg = Skill3EnvCfg()
    env_cfg.handoff_buffer_path = os.path.expanduser(args.handoff_buffer)
else:
    from lekiwi_nav_env import LeKiwiNavEnv, LeKiwiNavEnvCfg
    env_cfg = LeKiwiNavEnvCfg()

# AAC wrapper for skill-2/3, legacy는 symmetric
use_aac = args.skill in ("approach_and_grasp", "carry_and_place")
if use_aac:
    env = wrap_env_aac(raw_env)
else:
    env = wrap_env(raw_env, wrapper="isaaclab")

# AAC Models: CriticNet receives 37D/36D privileged obs
if use_aac and env._aac_state_space is not None:
    critic_obs_dim = env._aac_state_space.shape[0]
    models = {
        "policy": PolicyNet(env.observation_space, env.action_space, device),
        "value": CriticNet(
            env.observation_space, env.action_space, device,
            critic_obs_dim=critic_obs_dim,
        ),
    }
else:
    models = {
        "policy": PolicyNet(env.observation_space, env.action_space, device),
        "value": ValueNet(env.observation_space, env.action_space, device),
    }

# AAC Agent + Trainer:
if use_aac and env._aac_state_space is not None:
    agent = AAC_PPO(models=models, memory=memory, cfg=cfg_ppo,
                    observation_space=env.observation_space,
                    action_space=env.action_space, device=device,
                    critic_observation_space=env._aac_state_space)
    trainer = AACSequentialTrainer(cfg=trainer_cfg, env=env, agents=agent)
else:
    agent = PPO(models=models, memory=memory, cfg=cfg_ppo, ...)
    trainer = SequentialTrainer(cfg=trainer_cfg, env=env, agents=agent)
```

---

### 3-6. train_bc.py (277줄)

Behavioral Cloning 학습. `BCPolicy` 구조가 `PolicyNet`과 동일하여 weight transfer 가능.

> 전체 코드: `train_bc.py` 파일 직접 참조 (277줄).

**주요 기능:**
- `BCPolicy`: `net(obs→256→128→64) + mean_layer(64→9)` — PolicyNet과 동일
- `--expected_obs_dim` 필수: 30(Skill-2), 29(Skill-3) 검증
- 기본 정규화 OFF (PPO RunningStandardScaler와 정합)
- stale norm 파일 자동 삭제

```python
class BCPolicy(nn.Module):
    def __init__(self, obs_dim: int = 33, act_dim: int = 9):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 256),
            nn.ELU(),
            nn.Linear(256, 128),
            nn.ELU(),
            nn.Linear(128, 64),
            nn.ELU(),
        )
        self.mean_layer = nn.Linear(64, act_dim)

    def forward(self, obs: torch.Tensor) -> torch.Tensor:
        return self.mean_layer(self.net(obs))
```

---

### 3-7. collect_demos.py (1073줄)

RL Expert 데모 수집. `--skill` 분기 지원, camera + SpawnManager 통합.
**Audit Fix**: `Skill2EnvWithCam`, `Skill3EnvWithCam` 카메라 서브클래스 추가 (H1).

> 전체 코드: `collect_demos.py` 파일 직접 참조 (1073줄).

**주요 기능:**
- `--skill` 분기: approach_and_grasp / carry_and_place / legacy
- `extract_robot_state_9d()`: `[arm_pos(6), body_vel(3)]` 9D
  - body velocity: `root_lin_vel_b` / `root_ang_vel_b` 직접 읽기
- Gripper binary 변환: `action[5] = 1.0 if action[5] > 0.5 else 0.0`
- **Skill2EnvWithCam / Skill3EnvWithCam**: base_rgb (1280x720) + wrist_rgb (640x480) 카메라 서브클래스
- SpawnManager: 물체 스폰/리스폰, lighting DR
- HDF5 구조: obs, actions, robot_state, subtask_ids, images

```python
# 카메라 서브클래스 (Audit H1 수정):
class Skill2EnvWithCam(Skill2Env):
    """Skill2Env + base_rgb/wrist RGB 카메라 (VLA 데이터 수집용)."""
    def __init__(self, cfg, base_cam_w=1280, base_cam_h=720,
                 wrist_cam_w=640, wrist_cam_h=480, render_mode=None, **kwargs):
        self._base_cam_w, self._base_cam_h = base_cam_w, base_cam_h
        self._wrist_cam_w, self._wrist_cam_h = wrist_cam_w, wrist_cam_h
        super().__init__(cfg, render_mode, **kwargs)
    def _setup_scene(self):
        super()._setup_scene()
        # base_cam + wrist_cam 등록 (CameraCfg)

class Skill3EnvWithCam(Skill3Env):
    """Skill3Env + base_rgb/wrist RGB 카메라 (VLA 데이터 수집용)."""
    # 동일 구조

def extract_robot_state_9d(env) -> torch.Tensor:
    """VLA용 robot_state 9D:
    [arm_pos(5), gripper_pos(1), base_body_vel_x(1), base_body_vel_y(1), base_body_vel_wz(1)]
    단위: arm=rad, gripper=rad, base=m/s(x, y), rad/s(theta)
    """
    arm_pos = env.robot.data.joint_pos[:, env.arm_idx]   # 6D
    vx_body = env.robot.data.root_lin_vel_b[:, 0:1]   # x.vel (m/s)
    vy_body = env.robot.data.root_lin_vel_b[:, 1:2]   # y.vel (m/s)
    wz_body = env.robot.data.root_ang_vel_b[:, 2:3]   # theta.vel (rad/s)
    base_body_vel = torch.cat([vx_body, vy_body, wz_body], dim=-1)
    return torch.cat([arm_pos, base_body_vel], dim=-1)  # 9D
```

---

### 3-8. collect_navigate_data.py (601줄)

Skill-1 Navigate 데이터 수집. Script policy (P-control) + noise injection.

> 전체 코드: `collect_navigate_data.py` 파일 직접 참조 (601줄).

**주요 기능:**
- Directed Navigation (70%): P-control (K_LIN=0.8, K_ANG=1.5)
- Search Rotation (30%): 제자리 회전 또는 전진+회전
- Noise: 조향 흔들림 σ=0.05, 속도 양자화, 5% action repeat
- `Skill2EnvWithCam`: camera 지원 서브클래스
- Action: `[arm_rest 5D, gripper_open 1D, base_cmd 3D]` (v6)

```python
# Proportional controller gains
K_LIN = 0.8   # linear velocity gain
K_ANG = 1.5   # angular velocity gain

def compute_navigate_action(env, ep_mode, ...):
    # Directed: proportional control toward object
    directed_vx = (K_LIN * torch.cos(angle_to_target)).clamp(-max_lin, max_lin) / max_lin
    directed_vy = (K_LIN * torch.sin(angle_to_target)).clamp(-max_lin, max_lin) / max_lin
    directed_wz = (K_ANG * angle_to_target).clamp(-max_ang, max_ang) / max_ang

    # Action: [arm5, grip1, base3] lekiwi_v6
    action[:, 0:6] = arm_rest_action[:, 0:6]
    action[:, 5] = 1.0  # gripper open
    action[:, 6] = base_vx
    action[:, 7] = base_vy
    action[:, 8] = base_wz
```

---

### 3-9. convert_hdf5_to_lerobot_v3.py (525줄)

HDF5 → LeRobot v3 dataset 변환기.

> 전체 코드: `convert_hdf5_to_lerobot_v3.py` 파일 직접 참조 (525줄).

**주요 기능:**
- `convert_to_vla_units()`: passthrough (변환 불필요 — sim/real 모두 m/s, rad/s)
- Channel names: yubinnn11/lekiwi3 v3.0 호환
- Camera: `observation.images.front` / `observation.images.wrist`
- `infer_robot_state_from_obs()`: robot_state 없을 때 obs에서 추출 (30D/29D/37D/33D/24D)
- subtask 지원: tasks.parquet, subtasks.parquet

```python
# v3.0 채널 이름
state_names = [
    "arm_shoulder_pan.pos", "arm_shoulder_lift.pos", "arm_elbow_flex.pos",
    "arm_wrist_flex.pos", "arm_wrist_roll.pos", "arm_gripper.pos",
    "x.vel", "y.vel", "theta.vel",
]

def convert_to_vla_units(data_9d: np.ndarray) -> np.ndarray:
    """v3.0: sim velocity = real velocity — 단위 변환 불필요."""
    return data_9d.copy()

def infer_robot_state_from_obs(obs: np.ndarray) -> np.ndarray:
    dim = obs.shape[1]
    if dim == 30:  # Skill-2: arm(0:6) + base_body_vel(6:9)
        return np.concatenate([obs[:, 0:6], obs[:, 6:9]], axis=1)
    if dim == 29:  # Skill-3: arm(0:6) + base_body_vel(6:9)
        return np.concatenate([obs[:, 0:6], obs[:, 6:9]], axis=1)
    # ... legacy 33D/37D/24D fallback
```

---

### 3-10. record_teleop.py (766줄)

텔레옵 데모 녹화. ROS2/TCP 듀얼 입력, `--skill` 분기 지원.

> 전체 코드: `record_teleop.py` 파일 직접 참조 (766줄).

**주요 기능:**
- `--skill`: approach_and_grasp (Skill-2, 30D obs, v6 action) / legacy (v8 FSM)
- `--teleop_source`: auto/ros2/tcp
- `TcpTeleopSubscriber`: TCP JSON lines 직접 수신
- `Ros2TeleopSubscriber`: ROS2 topic 구독
- `teleop_to_action()`: v6/legacy 포맷 분기
- `normalize_arm_positions_to_rad()`: auto/rad/deg/m100 단위 감지
- robot_state 9D 동시 기록 (v3.0: body-frame velocity)
- Gripper binary 변환 (v6 모드)

```python
def teleop_to_action(arm_pos, body_cmd, ..., use_v6=False):
    action = np.zeros(9)
    if use_v6:
        # v6: [arm5, grip1, base3]
        action[0:6] = arm_norm
        action[6:9] = base_norm
    else:
        # legacy: [base3, arm6]
        action[0:3] = base_norm
        action[3:9] = arm_norm
    return action

# robot_state 9D 기록 (body-frame velocity)
arm_pos_state = env.robot.data.joint_pos[0, env.arm_idx].cpu().numpy()  # 6D
vx_body = env.robot.data.root_lin_vel_b[0, 0].item()
vy_body = env.robot.data.root_lin_vel_b[0, 1].item()
wz_body = env.robot.data.root_ang_vel_b[0, 2].item()
base_body_vel = np.array([vx_body, vy_body, wz_body])
episode_robot_state.append(np.concatenate([arm_pos_state, base_body_vel]))
```

---

### 3-11. AAC 구현 파일 (aac_wrapper.py + aac_ppo.py + aac_trainer.py)

skrl 1.4.3은 Asymmetric Actor-Critic을 네이티브로 지원하지 않는다.
IsaacLabWrapper는 `{"policy": 30D, "critic": 37D}` dict obs에서 "policy"만 추출하고
"critic"은 버린다. 이 3파일은 skrl 소스 수정 없이 AAC를 구현한다.

**아키텍처:**
```
env._get_observations() → {"policy": 30D, "critic": 37D}
                           ↓                    ↓
                    IsaacLabWrapper        self._critic_obs = critic_obs
                    (policy만 추출)              ↓
                           ↓              aac_wrapper.state()
                      actor obs               critic obs
                           ↓                    ↓
                    AAC_PPO.act()        AAC_PPO.record_transition()
                    (policy network)     (value network에 critic_states 전달)
```

**aac_wrapper.py (45줄):**
```python
def wrap_env_aac(env):
    """Isaac Lab 환경을 AAC 지원 wrapper로 감싼다."""
    wrapped = wrap_env(env, wrapper="isaaclab")
    raw_env = wrapped._unwrapped

    try:
        _state_space = raw_env.single_observation_space["critic"]
    except (KeyError, AttributeError):
        _state_space = None

    def state(self_wrapper):
        """현재 critic observation 반환."""
        return getattr(raw_env, "_critic_obs", None)

    import types
    wrapped.state = types.MethodType(state, wrapped)
    wrapped._aac_state_space = _state_space
    return wrapped
```

**aac_ppo.py (358줄):**
- `AAC_PPO(PPO)`: critic에 별도의 privileged observation 전달
- `__init__()`: `critic_states` memory 텐서 생성, `_critic_state_preprocessor` 초기화
- `record_transition()`: `_current_critic_states`로 value 계산, memory에 `critic_states` 저장
- `_update()`: full PPO._update() override — `sampled_critic_states`로 critic 학습

```python
class AAC_PPO(PPO):
    def __init__(self, ..., critic_observation_space=None):
        super().__init__(...)
        # critic_states memory tensor 생성
        self.memory.create_tensor(name="critic_states", size=critic_observation_space, ...)
        self._tensors_names = ["states", "critic_states", "actions", "rewards",
                               "terminated", "log_prob", "values"]

    def record_transition(self, ...):
        # critic obs로 value 계산
        values, _, _ = self.models["value"].act({"states": self._current_critic_states}, ...)
        self.memory.add_samples(critic_states=self._current_critic_states, ...)

    def _update(self, timestep, timesteps):
        # Mini-batch에서 critic_states 분리하여 value network에 전달
        sampled_critic_states = sampled_batches[1]  # (states, critic_states, actions, ...)
        predicted_values, _, _ = self.models["value"].act({"states": sampled_critic_states}, ...)
```

**aac_trainer.py (94줄):**
```python
class AACSequentialTrainer(SequentialTrainer):
    def single_agent_train(self):
        states, infos = self.env.reset()
        critic_states = self._get_critic_states()  # env.state() 호출

        for timestep in range(...):
            actions = self.agents.act(states, ...)
            next_states, rewards, terminated, truncated, infos = self.env.step(actions)
            next_critic_states = self._get_critic_states()

            # AAC: critic states를 agent에 주입
            self.agents._current_critic_states = critic_states
            self.agents._current_next_critic_states = next_critic_states
            self.agents.record_transition(...)

            states, critic_states = next_states, next_critic_states
```

---

### 3-12. deploy_vla_action_bridge.py (217줄)

VLA action(9D, [-1,1]) → real robot command 변환 유틸리티.
**Audit Fix**: `--action_format` v6/legacy 분기 추가 (H3).

> 전체 코드: `deploy_vla_action_bridge.py` 파일 직접 참조 (217줄).

**주요 기능:**
- `--action_format`: v6 (기본, `[arm5,grip1,base3]`) / legacy (`[base3,arm6]`)
- base action denormalization + sim→real transform
- arm action → arm target(rad) mapping

```python
# Action format branching (Audit H3 수정):
if args.action_format == "v6":
    # v6: [arm0-4, gripper, vx, vy, wz]
    arm_action = action[0:6]
    vx_sim = float(action[6] * max_lin_vel)
    vy_sim = float(action[7] * max_lin_vel)
    wz_sim = float(action[8] * max_ang_vel)
else:
    # legacy: [vx, vy, wz, arm0-5]
    vx_sim = float(action[0] * max_lin_vel)
    vy_sim = float(action[1] * max_lin_vel)
    wz_sim = float(action[2] * max_ang_vel)
    arm_action = action[3:9]
```

---

## 4. 실행 명령어 레퍼런스

### Skill-2 학습

```bash
# RL from scratch
python train_lekiwi.py --skill approach_and_grasp --num_envs 2048 \
    --multi_object_json object_catalog.json \
    --gripper_contact_prim_path "/World/envs/env_.*/Robot/Moving_Jaw_08d_v1" \
    --dynamics_json calibration/tuned_dynamics.json --headless

# BC -> RL Fine-tune
python train_lekiwi.py --skill approach_and_grasp --num_envs 2048 \
    --bc_checkpoint checkpoints/bc_nav.pt \
    --multi_object_json object_catalog.json \
    --gripper_contact_prim_path "/World/envs/env_.*/Robot/Moving_Jaw_08d_v1" \
    --dynamics_json calibration/tuned_dynamics.json --headless
```

### Handoff Buffer 생성

```bash
python generate_handoff_buffer.py \
    --checkpoint logs/ppo_approachandgrasp/best_agent.pt \
    --num_entries 500 --num_envs 64 \
    --output handoff_buffer.pkl \
    --multi_object_json object_catalog.json \
    --gripper_contact_prim_path "/World/envs/env_.*/Robot/Moving_Jaw_08d_v1" \
    --dynamics_json calibration/tuned_dynamics.json --headless
```

### Skill-3 학습

```bash
python train_lekiwi.py --skill carry_and_place --num_envs 2048 \
    --handoff_buffer handoff_buffer.pkl \
    --multi_object_json object_catalog.json \
    --gripper_contact_prim_path "/World/envs/env_.*/Robot/Moving_Jaw_08d_v1" \
    --dynamics_json calibration/tuned_dynamics.json --headless
```

### Expert 데모 수집

```bash
# Skill-2 expert (카메라 포함)
python collect_demos.py --skill approach_and_grasp \
    --checkpoint logs/ppo_approachandgrasp/best_agent.pt \
    --num_demos 2000 --num_envs 4 \
    --multi_object_json object_catalog.json \
    --gripper_contact_prim_path "/World/envs/env_.*/Robot/Moving_Jaw_08d_v1" \
    --dynamics_json calibration/tuned_dynamics.json

# Navigate script policy
python collect_navigate_data.py --num_demos 1000 --num_envs 4 \
    --multi_object_json object_catalog.json \
    --gripper_contact_prim_path "/World/envs/env_.*/Robot/Moving_Jaw_08d_v1" \
    --dynamics_json calibration/tuned_dynamics.json
```

### LeRobot 변환

```bash
python convert_hdf5_to_lerobot_v3.py \
    --input outputs/rl_demos/*.hdf5 \
    --output_root ~/datasets/lekiwi_skill2_v3 \
    --repo_id yubinnn11/lekiwi3 --overwrite
```

### 텔레옵 녹화

```bash
# Skill-2 텔레옵
python record_teleop.py --num_demos 20 --skill approach_and_grasp \
    --multi_object_json object_catalog.json \
    --gripper_contact_prim_path "/World/envs/env_.*/Robot/Moving_Jaw_08d_v1" \
    --dynamics_json calibration/tuned_dynamics.json
```

---

## 5. Codefix 반영 현황

### 5-1. codefix.md (이전 세션)

| # | 수정 내용 | 파일 | 상태 |
|---|----------|------|------|
| 1 | Gripper body name `"Moving_Jaw_08d_v1"` | `lekiwi_skill2_env.py:450` | 완료 |
| 2 | Critic obs: `gripper_rel_pos(3)` | `lekiwi_skill3_env.py:19-20,156-168` | 완료 |
| 3 | `--skill` 분기 (record_teleop.py) | `record_teleop.py:123-128` | 완료 |
| 4 | v6 action format (record_teleop.py) | `record_teleop.py:489,420-423` | 완료 |
| 5 | robot_state body-frame velocity | `record_teleop.py:668-674` | 완료 |
| 6 | `object_ori` in handoff buffer | `generate_handoff_buffer.py:108-123` | 완료 |
| 7 | `_handoff_object_ori` buffer | `lekiwi_skill3_env.py:88-90` | 완료 |
| 8 | object_ori read from entry | `lekiwi_skill3_env.py:284-287` | 완료 |
| 9 | object_ori write to sim | `lekiwi_skill3_env.py:315` | 완료 |

### 5-2. AUDIT_REPORT 반영 (현재 세션)

| ID | 심각도 | 제목 | 수정 파일 | 상태 |
|----|--------|------|----------|------|
| C1 | Critical | place_success 도달 불가 | `lekiwi_skill3_env.py` | 완료 — `_update_grasp_state()` override, intentional place 메커니즘 |
| C2 | Critical | AAC critic에 actor obs 전달 | `aac_wrapper.py`, `aac_ppo.py`, `aac_trainer.py`, `train_lekiwi.py`, `lekiwi_skill2_env.py`, `lekiwi_skill3_env.py`, `generate_handoff_buffer.py` | 완료 — 3파일 AAC 구현 |
| H1 | High | Skill-2/3 camera subclass 누락 | `collect_demos.py` | 완료 — `Skill2EnvWithCam`, `Skill3EnvWithCam` |
| H2 | High | Curriculum dead code | `lekiwi_skill2_env.py` | 완료 — config에서 `curriculum_current_max_dist` 읽기 |
| H3 | High | Deploy action 순서 하드코딩 | `deploy_vla_action_bridge.py` | 완료 — `--action_format` v6/legacy 분기 |
| M1 | Medium | 3D obs 중복 (lin_vel/ang_vel + base_body_vel) | — | 스킵 — 학습 결과 영향 없음 |
| M2 | Medium | contact_lr fake 2D | — | 스킵 — 학습 결과 영향 없음 |
| M4 | Medium | train_bc action 이름 legacy 순서 | `train_bc.py` | 완료 — v6 순서로 수정 |
| L3 | Low | Handoff buffer 로그 누락 | `generate_handoff_buffer.py` | 완료 — milestone 기반 로깅 |

### 5-3. INDEPENDENT_AUDIT_REPORT 반영

| ID | 심각도 | 제목 | 수정 파일 | 상태 |
|----|--------|------|----------|------|
| CRITICAL-1 | Critical | `grasp_gripper_threshold=-0.3` 도달 불가 | `lekiwi_skill2_env.py`, `train_lekiwi.py`, `collect_demos.py`, `record_teleop.py`, `lekiwi_nav_env.py` | 완료 — 0.7으로 변경 (물체 파지 시 0.4~0.8 rad 대응) |
| CRITICAL-2 | Critical | Skill-3 에피소드 1 step 만에 종료 | `lekiwi_skill3_env.py` | 완료 — `_get_dones()`에서 place_success로 task_success 오버라이드 |
| HIGH-1 | High | `compute_gae()` next_values 무시 | `aac_ppo.py` | 완료 — `last_values` → `next_values` 파라미터 사용 |
| HIGH-2 | High | 문서-코드 불일치 (threshold) | `pipeline_new/*.md` | 완료 — 모든 문서에서 -0.3 → 0.7으로 수정 |
| NEW-3 | - | Gripper shaping reward 항상 0 | — | CRITICAL-1 수정으로 자동 해결 |
| NEW-5 | - | Per-loop tensor creation 성능 | `lekiwi_skill3_env.py` | 완료 — 배치 텐서 생성으로 변경 |
| MEDIUM-1 | Medium | base_body_vel obs 중복 | — | 스킵 — 의도적, 학습 결과 영향 없음 |
| MEDIUM-2 | Medium | DR 전체 env 적용 | — | 스킵 — 기능적 오류 아님 |
| MEDIUM-3 | Medium | contact_lr fake L/R | — | 스킵 — 의도적, 학습 결과 영향 없음 |
| FEEDBACK-1 | Medium | grasp_gripper_threshold 0.3→0.7 | 5 files + docs | 완료 — 일반 물체 파지 대응 |
| FEEDBACK-2 | Medium | Single-object sim write 누락 | lekiwi_skill3_env.py | 완료 — _reset_from_handoff + _reset_fallback |
| FEEDBACK-3 | Medium | Handoff buffer per-load noise | lekiwi_skill3_env.py | 완료 — arm/base_pos/base_yaw noise 3종 |
| SIM2REAL-1 | High | DR 범위 확대 | lekiwi_skill2_env.py | 완료 — damping 0.3~3.0x, mass 0.5~2.0x |
| SIM2REAL-2 | High | Observation noise | lekiwi_skill2_env.py, lekiwi_skill3_env.py | 완료 — joint/vel/rel noise 3종 |
| SIM2REAL-3 | Medium | Action delay | lekiwi_skill2_env.py | 완료 — 1 step delay buffer |
| SIM2REAL-4 | Medium | Action smoothness penalty | lekiwi_skill2_env.py, lekiwi_skill3_env.py | 완료 — -0.005 × delta² |
| SIM2REAL-5 | Medium | Tanh proximity kernel | lekiwi_skill2_env.py | 완료 — 2.0 × (1-tanh(d/0.5)) |
| SIM2REAL-6 | Low | Orthogonal weight init | models.py | 완료 — PPO 37 details |
| SIM2REAL-7 | Low | PPO ratio_clip/grad_norm_clip | train_lekiwi.py | 완료 — 0.15/0.5 |

### 5-4. AUDIT V2 반영

| ID | 심각도 | 제목 | 수정 파일 | 상태 |
|----|--------|------|----------|------|
| AUDIT-V2-1 | Critical | Handoff buffer 좌표계: 절대→상대 좌표 저장 | `generate_handoff_buffer.py` | 완료 |
| AUDIT-V2-2 | Critical | _reset_from_handoff: env_origin 복원 + home_pos_w 수정 | `lekiwi_skill3_env.py` | 완료 |
| AUDIT-V2-3 | High | _finish_reset: _action_delay_buf 리셋 추가 | `lekiwi_skill3_env.py` | 완료 |
| AUDIT-V2-4 | Low | _reset_from_handoff: object velocity 초기화 (방어적) | `lekiwi_skill3_env.py` | 완료 (FixedJoint이 처리하므로 방어적) |
| AUDIT-V2-5 | High | curriculum window 클리어 (난이도 상승 시 연쇄 방지) | `lekiwi_skill2_env.py` | 완료 |
| ~~AUDIT-V2-5b~~ | — | ~~prev_object_dist 실제 거리로 초기화~~ | — | 비적용 (상수 오프셋, advantage 정규화로 상쇄) |
| ~~AUDIT-V2-6~~ | — | ~~clip_predicted_values=False~~ | — | 비적용 (하이퍼파라미터 선택, 버그 아님) |

---

## 6. 남은 작업

- [x] ~~skrl PPO에서 AAC critic obs 별도 저장 → CriticNet 전환~~ (완료: aac_*.py 3파일)
- [x] ~~Skill-2 camera subclass (collect_demos.py approach_and_grasp + camera)~~ (완료: Skill2EnvWithCam)
- [x] ~~Skill-3 camera subclass~~ (완료: Skill3EnvWithCam)
- [x] ~~place_success 도달 불가 수정~~ (완료: intentional place 메커니즘)
- [ ] Skill-2 RL 학습 실행 및 성능 검증
- [ ] Handoff Buffer 생성
- [ ] Skill-3 RL 학습 실행 및 성능 검증
- [ ] Expert 데모 수집 (Skill-1/2/3)
- [ ] LeRobot 변환 및 VLA 파인튜닝
- [ ] 실기 배포 테스트
