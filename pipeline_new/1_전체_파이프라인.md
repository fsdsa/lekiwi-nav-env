# LeKiwi Mobile Manipulator — 전체 파이프라인

---

## 1. 우리가 풀려는 문제

"빨간 컵 가져와"라고 말하면 로봇이 스스로 이동하고, 컵을 찾아 집고, 돌아와서 내려놓는 시스템을 만든다. 이것은 단순한 pick-and-place가 아니다. 로봇이 현재 위치에서 목표 물체가 있는 곳까지 이동해야 하고, 이동하면서 팔을 뻗어 물체를 잡아야 하고, 물체를 안 떨어뜨리면서 원래 위치로 돌아와야 한다. 이동과 조작이 동시에 일어나는 mobile manipulation이다.

현재 공개된 VLA 모델들은 대부분 고정된 팔이 테이블 위 물체를 집는 수준이다. 베이스 이동과 팔 조작을 동시에 하는 작업에 대한 학습 데이터도, 검증된 파이프라인도 부족하다. 이 파이프라인은 그 간극을 메운다.

핵심 아이디어: sim에서 RL로 skill별 최고의 행동을 만들고, 대량 rollout으로 VLA 학습 데이터를 생산하고, VLA에 distillation한 뒤, 실제 로봇에서 VLM이 상위 오케스트레이션을 하면서 VLA가 실행한다.

---

## 2. 로봇: LeKiwi

### 2-1. 물리적 구성

LeKiwi는 HuggingFace LeRobot 프레임워크와 완전 호환되는 오픈소스 모바일 매니퓰레이터다.

팔은 SO-100이다. Feetech STS3215 서보 6개가 달려 있고, 5개가 관절(shoulder_pan, shoulder_lift, elbow_flex, wrist_flex, wrist_roll), 1개가 그리퍼(open/close)다. 모두 12-bit 자기 엔코더가 내장되어 현재 위치를 정밀하게 읽는다. reach 약 40cm, 페이로드 600~1000g.

베이스는 옴니휠 3개의 Kiwi Drive다. holonomic이라서 전후좌우 어디로든 이동할 수 있고 제자리 회전도 된다. 모터 ID 7, 8, 9가 바퀴에 할당된다.

단일 Motor Control Board에 arm 6개 + wheel 3개 = 총 9개 모터가 연결된다.

### 2-2. 센서

base_cam은 Intel RealSense D455다. RGB-D 카메라와 IMU가 일체형으로 들어 있다. RGB 스트림은 1280×720 30fps로 VLA 입력과 VLM 상황 판단에 쓴다. Depth 스트림은 최대 4m 거리에서 2% 이내 오차로, Safety Layer의 전방 장애물 감지에 쓴다. 내장 IMU(6축)는 VIO(Visual-Inertial Odometry)에 쓴다. FOV 약 86°(H).

wrist_cam은 그리퍼 근처 USB 웹캠으로, 640×480 30fps. 정밀 파지에 쓴다.

둘 다 Raspberry Pi 5에 연결, ZeroMQ로 리모트 PC에 전송된다.

모터 엔코더 9개: arm 6개(12-bit 자기 엔코더, Feetech STS3215 내장) + wheel 3개. 휠 엔코더에서 Kiwi FK를 통해 body-frame velocity를 계산한다.

**없는 센서**: GPS, LiDAR, AprilTag, 접촉 센서, 힘 센서. 360° 장애물 감지가 필요할 경우 RPLiDAR A1 추가를 고려하지만, 초기 검증에서는 D455의 전방 depth + 보수적 속도 제한으로 충분하다.

**D455가 주는 것과 주지 않는 것**: D455 덕분에 depth와 IMU가 있어서, 전방 장애물 감지와 VIO 기반 위치 추적이 가능하다. 그러나 접촉 센서와 힘 센서는 여전히 없으므로 그리퍼가 물체에 닿았는지, 얼마나 세게 잡고 있는지는 알 수 없다. 이것이 RL 학습에서 contact과 grip_force를 privileged obs로 sim ground truth에서 가져오는 이유다.

### 2-3. 베이스 상태 계산

**데이터셋 형식 (확정)**: 실제 로봇 데이터(`yubinnn11/lekiwi3`, LeRobot v3.0)의 observation.state[6:9]는 `x.vel, y.vel, theta.vel`로 명명되어 있으며, **body-frame velocity(m/s, rad/s)**를 기록한다. 초기에 참조한 `theo-michel/lekiwi_v6`(v2.1)은 `x_mm, y_mm, theta`(displacement, mm)였으나, 실제 데이터 확인 결과 v3.0 velocity 포맷이 확정되었다.

**우리 파이프라인의 선택: body-frame velocity**. VLA에 입력되는 base state는 매 프레임의 body-frame 속도(vx m/s, vy m/s, wz rad/s)이다. sim에서는 Isaac Sim의 `root_lin_vel_b`와 `root_ang_vel_b`에서 직접 읽으며, 실제 로봇에서는 휠 엔코더 → Kiwi FK → body velocity로 계산한다. sim과 real 모두 동일 단위(m/s, rad/s)이므로 단위 변환이 불필요하다.

**전형적 값 범위** (yubinnn11/lekiwi3 실측): x.vel: -0.03~0.08 m/s, y.vel: -0.10~0.01 m/s, theta.vel: 이산적 ±0.5859, ±1.1719, ±1.7578 rad/s.

body_pos(절대 위치)를 VLA의 9D state에 넣지 않는 이유: D455의 VIO + 휠 odometry + EKF fusion으로 approximate pose를 추정할 수 있지만(후술), 이것을 VLA의 state에 넣으면 sim에서도 동일한 drift 특성을 재현해야 한다. sim의 perfect position과 real의 drift 있는 position 사이의 gap이 sim-to-real 전이를 깨뜨릴 수 있다. 대신 위치 정보는 VLM에게 텍스트로 전달하여 instruction에 반영하는 방식을 취한다(섹션 4-8 참조).

### 2-4. 위치 추적: VIO + Wheel Odometry + EKF Fusion

D455의 stereo + IMU로 VIO(Visual-Inertial Odometry)를 돌리고, 휠 엔코더 odometry와 Extended Kalman Filter로 융합하면, 실내에서 수 분 동안 수십 cm 수준의 drift로 위치를 추적할 수 있다.

이 위치 추정이 활용되는 곳: VLM 루프에서 position context 활용, 탐색 맵 구축, CarryAndPlace에서 home 복귀 방향 판단.

Jetson Orin Nano Super에서 D455 VIO를 실시간 처리하는 것은 흔한 구성이라 구현이 현실적이다. 다만 이 기능은 Phase 5(실배포)에서 구현하며, Phase 1~3(sim RL 학습 + 데이터 수집)에는 영향을 주지 않는다.

---

## 3. Observation & Action Space — 9D

### 3-1. 구성

실물 LeKiwi 데이터셋(`yubinnn11/lekiwi3`, LeRobot v3.0)을 보면 observation.state와 action 모두 shape [9]이다.

```
VLA 데이터 순서 (yubinnn11/lekiwi3 v3.0 호환):
  [arm_shoulder_pan.pos, arm_shoulder_lift.pos, arm_elbow_flex.pos, arm_wrist_flex.pos, arm_wrist_roll.pos, arm_gripper.pos, x.vel, y.vel, theta.vel]
   ─── arm 5D (joint position, rad) ──────────────────────────── grip 1D ── base velocity 3D (m/s, rad/s) ──
```

이것이 VLA에 입력되는 state의 전부다. action도 동일한 9D다. arm joint target 5D + gripper target 1D + base command 3D.

**⚠ v8 RL 환경의 action 순서는 다르다.** 기존 `lekiwi_nav_env.py`의 action은 `[0:3]=base_vel_cmd(vx, vy, wz), [3:9]=arm_pos_target(6D, gripper 포함)` 순서다. 이것은 VLA 포맷의 `[arm5, grip1, base3]`과 **정반대**다. 새 skill 환경에서는 VLA/yubinnn11/lekiwi3 순서 `[arm5, grip1, base3]`으로 통일한다. 이렇게 해야 수집 데이터를 별도 remapping 없이 바로 VLA 학습에 쓸 수 있다. v8의 Kiwi IK, arm position target 적용 로직은 인덱싱만 바꿔서 재사용한다.

**⚠ v8의 robot_state도 VLA 포맷과 다르다.** 기존 `collect_demos.py`의 `extract_robot_state_9d()`는 `arm_joint_pos(6) + wheel_angular_vel(3)`을 저장한다. 그런데 yubinnn11/lekiwi3는 `arm_joint_pos(6) + body_frame_velocity(3)`이다. 마지막 3D가 완전히 다른 물리량(개별 휠 각속도 vs body-frame 속도)이다. 새 skill 환경에서는 Isaac Sim의 `root_lin_vel_b`와 `root_ang_vel_b`에서 body-frame velocity(vx, vy, wz)를 직접 읽어 저장한다. 단위 변환 불필요 (sim m/s = real m/s).

**gripper 표현**: RL 학습 시에는 **continuous position target**(v8과 동일)을 사용한다. binary 0/1로 바꾸면 그리퍼를 서서히 닫아가면서 물체를 감싸는 행동을 학습할 수 없다. VLA 데이터 저장 시에만 0.5 threshold로 binary 변환한다.

### 3-2. Semantic 분리 (GR00T 호환)

GR00T N1.6의 modality config 체계에 맞추면 같은 9D를 이렇게 분리한다:

- state: single_arm(5D) + gripper(1D) + base(3D)
- action: single_arm(5D) + gripper(1D) + base(3D)

π0-FAST는 이런 분리 없이 flat [9] 그대로 받는다.

### 3-3. RL 학습용 Observation은 다르다

RL Expert 학습 시에는 sim에서만 얻을 수 있는 privileged 정보를 Actor와 Critic에 추가한다. 물체의 정확한 상대 위치, 그리퍼 접촉 여부, home 방향 같은 정보다. 이것들은 VLA에 전달되지 않는다. VLA는 카메라 이미지만 보고 이 정보를 스스로 추론해야 하며, 그것이 distillation의 핵심이다.

**Skill별 RL obs가 다르다.** Skill-1(Navigate) Actor는 20D: arm(5) + grip(1) + base_body_vel(3) + rel_object(3) + pseudo-lidar(8). Critic은 25D: Actor 20D + dist(1) + heading(1) + vel_toward(1) + closest_obs_dist(1) + closest_obs_angle(1). Skill-2(ApproachAndGrasp) Actor는 30D: 9D + base_vel(6) + arm_vel(6) + rel_object(3) + contact(2) + obj_bbox(3) + obj_category(1). Skill-3(CarryAndPlace) Actor는 29D: 9D + base_vel(6) + arm_vel(6) + home_rel(3) + grip_force(1) + obj_bbox(3) + obj_category(1). Skill-2/3은 물체 크기/종류(bbox, category)를 포함하여 12종 다중 물체에 대한 차별화된 전략을 학습한다. **속도 정보(base_vel 6D + arm_vel 6D = 12D)는 v8에서도 obs에 포함되어 있으며, 동적 제어에 필수다.** 이것을 빼면 Actor가 현재 운동 상태를 알 수 없어서 과도한 가감속, 오버슈트를 발생시킨다. 상세한 obs 구성은 Sim 데이터 수집 파이프라인 문서에서 다룬다.

---

## 4. 실행 구조: VLM + VLA Dual-System

### 4-1. 왜 두 개가 필요한가

"빨간 컵 가져와"는 여러 단계로 쪼개진다. VLA가 한 번에 예측하는 action chunk는 10~16 timestep(0.3~0.5초)인데, 전체 task는 수백 timestep(수십 초~수 분)이 걸린다.

더 근본적으로, VLA는 "어떻게 움직일지(how)"를 잘하지만 "지금 뭘 해야 하는지(what)"를 판단하는 것은 약하다. 물체가 보이는지, 잡았는지, 어디로 가야 하는지 같은 상위 판단은 언어-시각 추론이 필요하고, 이것은 VLM이 훨씬 잘한다.

그래서 VLM이 "what"을, VLA가 "how"를 담당한다.

### 4-2. VLM: Qwen2.5-VL-7B-Instruct

Qwen2.5-VL은 ViT vision encoder + Qwen2.5 LLM decoder 구조의 7B 파라미터 VLM이다. A100에서 bf16 기준 ~15GB VRAM. Apache 2.0 라이선스.

역할: 카메라 이미지를 보고 현재 상황을 판단한 뒤, VLA에게 줄 자연어 instruction을 생성한다. 직접 모터를 제어하지 않는다.

### 4-3. VLA: π0-FAST 또는 GR00T N1.6

VLA는 5~10Hz로 카메라 이미지 두 장 + VLM이 준 instruction 텍스트 + 현재 robot_state 9D를 받아서 9D action chunk를 생성한다.

### 4-4. VLM 루프: 시스템의 핵심 흐름

VLM은 한 번 instruction을 내리고 끝이 아니다. 약 3초마다(0.3Hz) base_cam 이미지를 받아서 상황을 판단하고 새 instruction을 생성한다. 그 사이 VLA는 이전 instruction을 계속 실행한다(5~10Hz로 action chunk 생성).

VLM에게 주는 system prompt 구조:

```
너는 모바일 매니퓰레이터의 지휘자다.
사용자 요청: "{user_request}"

현재 카메라 이미지를 보고, 로봇이 다음에 해야 할 행동 하나를 자연어로 지시해라.

상황 판단 기준:
- 목표 물체가 이미지에 보이지 않으면 → 탐색 지시 (방향 회전 또는 전진 탐색)
- 목표 물체가 멀리 보이면 → 접근 지시 (물체 방향으로 이동)
- 목표 물체가 팔이 닿을 거리에 보이면 → 파지 지시 (다가가서 잡기)
- 물체를 이미 잡고 있으면 → 운반 지시 (목표 위치로 이동 후 놓기)
- 작업이 완료되었으면 → "done"

출력: 로봇에게 줄 한 문장의 영어 지시문
```

### 4-5. 물체를 못 찾을 때

VLM 루프가 자연스럽게 해결한다. 이미지에 빨간 컵이 안 보이면 "turn right slowly to search"를 내린다. VLA가 제자리 회전. 다음 3초 후에도 안 보이면 "move forward and look around". 이런 식으로 VLM이 탐색 행동을 반복 지시한다. 별도의 "Search" skill이 아니다.

### 4-6. Navigate → ApproachAndGrasp 전환 기준

VLM이 이미지에서 물체의 시각적 크기로 대략 판단한다. ApproachAndGrasp 자체가 base+arm 동시 제어이기 때문에 전환 시점이 정밀하지 않아도 된다. Curriculum에서 0.5m~2.5m 범위로 학습하므로 그 안에만 들어오면 된다.

### 4-7. 완료 판단

VLM이 이미지를 보고 "물체가 목표 위치에 놓여있고 그리퍼가 비어있다"를 확인하면 "done"을 출력한다.

### 4-8. VLM에 위치 맥락 전달

VIO + Wheel Odometry + EKF fusion(섹션 2-4)으로 추정한 approximate pose를 VLM의 system prompt에 텍스트로 주입한다.

```
추가 맥락:
- 현재 위치: home에서 약 2.1m, 방위 NE (약 42°)
- 탐색 현황: 왼쪽 180° 탐색 완료, 오른쪽 미탐색
```

위치 맥락 없이도 VLM 루프는 동작한다. Phase 5(실배포)에서 VIO가 구현되면 활성화한다.

---

## 5. 3-Skill 분리 설계

### 5-1. 세 가지 Skill

**Navigate**: 목표 방향으로 이동하거나, 물체를 탐색하며 돌아다님. base만 움직이고 arm은 tucked pose 유지. **장애물 회피 + 감속 정지 행동을 학습하기 위해 RL(AAC-PPO)을 사용한다.** 텐서 기반 랜덤 장애물과 8-ray pseudo-lidar를 포함하는 `lekiwi_skill1_env.py`에서 학습. Actor 20D obs / Critic 25D obs.

**ApproachAndGrasp**: 물체에 base+arm 동시 접근하여 파지. 이동과 조작이 동시에 일어나는 가장 어려운 부분. 에피소드 시작 시 **물체는 로봇 전방(카메라 FOV 내)에 배치**된다 (`spawn_object_in_front=True`, 로봇 yaw를 물체 방향 + Gaussian noise σ=0.35rad로 설정). Navigate가 물체 근처까지 이동한 상태를 가정하기 때문이다.

**CarryAndPlace**: 물체를 잡은 채 home으로 이동하여 배치. 안 떨어뜨리면서 이동하고 내려놓기.

### 5-2. 왜 3개로 나누나

첫째, RL 학습 효율. 전체를 하나의 RL로 학습하면 reward가 너무 sparse하고 horizon이 너무 길어서 수렴하지 않는다. 쪼개면 각 skill이 짧고 reward를 밀도 있게 줄 수 있다.

둘째, ApproachAndGrasp와 CarryAndPlace의 reward 구조와 reset 분포가 근본적으로 다르다. 전자는 "물체까지 거리 줄이기 → 접촉 → 잡기"이고, 후자는 "home까지 거리 줄이기 → 물체 안 떨어뜨리기 → 내려놓기"다.

셋째, 데이터 수집과 VLA 학습의 유연성. skill별로 데이터를 따로 모으면 특정 skill만 보강하거나 교체하기 쉽다.

**기존 v8 단일 FSM(SEARCH→APPROACH→GRASP→RETURN)과의 관계**: v8 코드의 `lekiwi_nav_env.py`에 구현된 37D obs + 4-phase FSM은 전체 task를 단일 환경에서 처리한다. 3-Skill 분리는 이 FSM을 skill별 독립 환경으로 리팩토링하는 것이다. v8의 물리 grasp, dynamics DR, 캘리브레이션 연동 코드는 skill별 환경에서 그대로 재사용한다.

### 5-3. Navigate도 RL을 사용하는 이유

Navigate는 holonomic base라서 목표 방향 이동 자체는 단순하지만, **스크립트 정책(P-controller)으로는 장애물 회피와 목표 근처 감속 정지를 학습할 수 없다.** VLA가 이런 행동 패턴을 한 번도 보지 못하면 실환경에서 물체/가구에 충돌하거나 목표를 오버슈트한다.

따라서 Navigate도 RL(AAC-PPO)로 학습한다. `lekiwi_skill1_env.py`에서 텐서 기반 랜덤 장애물(3~8개) + 8-ray pseudo-lidar로 장애물 회피와 감속 행동을 학습한다. arm은 TUCKED_POSE 고정, gripper는 open 고정이므로 9D action 중 base 3D만 유효하다. BC warm-start 없이 from scratch로 학습 가능 (3D holonomic base는 랜덤 탐색만으로 충분히 reward를 받는다).

Navigate 데이터 수집은 `collect_demos.py --skill navigate`로 RL Expert rollout을 사용한다. 기존의 `collect_navigate_data.py`(스크립트 정책)도 fallback으로 유지된다.

### 5-4. 통일된 9D Action

모든 skill의 데이터가 동일한 9D action으로 기록된다. Navigate에서도 arm 5D는 tucked pose 유지 target, gripper는 open(1.0), base 3D는 이동 command로 9D action을 기록한다.

---

## 6. 데이터 부족 문제와 해결: 텔레옵 → BC → RL → Expert Rollout

### 6-1. 문제

VLA를 파인튜닝하려면 task당 수백~수천 에피소드의 (이미지, instruction, state, action) 쌍이 필요하다. 사람이 텔레옵으로 이걸 모으는 건 비현실적이다.

### 6-2. 해결: sim을 데이터 공장으로

Isaac Sim에서 RL로 expert 정책을 학습하고, 그 expert를 수천 번 자동 실행하면서 데이터를 대량 생산한다. Domain Randomization으로 다양성을 확보하면, sim 데이터만으로도 real에서 동작하는 VLA를 학습할 수 있다.

### 6-3. RL을 처음부터 돌릴 수 없는 문제

9D action space에서 랜덤 행동으로 물체를 잡을 확률은 거의 0이다. PPO를 from scratch로 돌리면 reward를 한 번도 못 받고 수렴이 안 된다.

### 6-4. 해결: 텔레옵 → BC → RL 부트스트래핑

**1단계: 텔레옵.** 사람이 sim에서 skill당 10~20개 시범을 보인다. 기존 `record_teleop.py` (ROS2/TCP 듀얼 모드, arm 단위 자동 감지) + `leader_to_home_tcp_rest_matched_with_keyboard_base.py` (Windows SO-100 리더 + 키보드 TCP 전송)를 사용한다.

**2단계: BC(Behavior Cloning).** 텔레옵 데이터로 순수 supervised learning. 10~20개로 성공률 30~40%면 잘 된 거다. 중요한 건 랜덤보다 훨씬 낫다는 것.

BC에서 핵심적인 설계 포인트: BC는 sim 안에서 실행하므로, 기록 시점에 9D state + sim ground truth(물체 상대 위치, 접촉 여부, 물체 크기/종류 등)를 함께 수집하여 RL Actor input과 동일한 형태를 만든다. **BC의 네트워크 구조(hidden dims, activation)가 RL Actor와 정확히 같아야 한다.** 기존 `models.py`의 `PolicyNet`(ELU activation, hidden dims 256→128→64)을 공유한다. 물체 크기(obj_bbox)와 종류(obj_category)를 Actor에 포함시켜서 다중 물체에 대해 물체별 차별화된 grasp/carry 전략을 학습할 수 있게 한다 (obs 정규화 시 `num_object_categories=6` 기준). **Orthogonal initialization (PPO 37 details)**: hidden layer는 gain=sqrt(2), policy output layer는 gain=0.01, value output layer는 gain=1.0으로 초기화한다. 이는 PPO 수렴 안정성을 높이는 표준 설정이며 `models.py`의 `PolicyNet`/`CriticNet` 모두에 적용한다.

기존 구현과의 관계: `train_bc.py`는 `--normalize` OFF가 기본(PPO RunningStandardScaler와 호환). BC checkpoint 구조는 `net.0/2/4.weight/bias + mean_layer.weight/bias` (log_std_parameter 없음). `train_lekiwi.py`가 BC weights를 PolicyNet에 로드하고 log_std_parameter를 `-0.5`로 설정한다 (BC는 deterministic이므로 log_std가 없고, RL 탐색을 위해 기본 `-1.0`보다 높은 `-0.5`를 사용).

**3단계: RL.** BC checkpoint로 PPO의 policy network를 초기화하고 RL 학습을 시작한다. BC가 30%였던 것이 RL을 거치면 90% 이상까지 올라간다.

```
텔레옵 (10~20개, 사람 수준)
  → BC (supervised, ~30% 성공률, warm start용)
    → RL (PPO + AAC, 90%+ 성공률, expert 수준)
      → Expert rollout (1K~10K 에피소드 대량 생산)
        → LeRobot 포맷 변환
          → VLA 파인튜닝
```

사람 텔레옵이 필요한 것은 Skill-2, Skill-3의 BC warm start용 10~20개씩뿐이다. Navigate는 사람 손이 전혀 들어가지 않는다.

---

## 7. RL 학습의 핵심 설계

### 7-1. Asymmetric Actor-Critic (AAC)

Actor와 Critic이 서로 다른 observation을 받는다.

Actor는 sim에서만 얻을 수 있는 privileged 정보를 포함하여 **가능한 한 최고의 행동**을 만든다. 물체의 정확한 상대 위치, 그리퍼 접촉 여부, 물체 크기/종류(bbox, category) 같은 정보다. 카메라 이미지는 Actor에 입력하지 않는다 — privileged state가 이미 충분한 정보를 제공하고, 이미지를 넣으면 CNN/vision encoder가 필요해서 병렬 환경 수가 급감한다. 이미지는 Phase 2 데이터 수집 시 렌더링하여 저장만 한다.

물체 bbox/category를 Actor에 포함시키는 이유: 기존 v8에서 검증된 핵심 설계다. 12종 다중 물체를 학습할 때, Actor가 물체 크기와 형태를 알아야 물체별로 다른 접근 각도, arm trajectory, gripper timing을 학습할 수 있다.

Critic은 Actor obs에 더해서 물체 바운딩박스, 질량 등 더 많은 privileged 정보를 받는다. Skill-1 Critic은 25D(Actor 20D + dist/heading/vel_toward/closest_obs_dist/closest_obs_angle 5D), Skill-2 Critic은 37D(Actor 30D + bbox_full 3D + mass 1D + dist/heading/vel 3D), Skill-3 Critic은 36D(Actor 29D + bbox 3D + mass 1D + gripper_rel_pos 3D).

**AAC 구현**: skrl 1.4.3는 native AAC를 지원하지 않는다(IsaacLabWrapper가 "policy" key만 추출하고 "critic"을 버림, PPO가 actor/critic에 동일 obs 전달). 이를 3개 파일로 우회한다: `aac_wrapper.py`가 IsaacLabWrapper에 `state()` 메서드를 monkey-patch하여 critic obs를 노출하고, `aac_ppo.py`가 PPO를 상속하여 `critic_states` memory tensor를 관리하며, `aac_trainer.py`가 SequentialTrainer를 상속하여 critic states를 매 step 추적한다. 환경의 `_get_observations()`는 `self._critic_obs`에 critic obs를 저장하고, wrapper의 `state()`가 이를 읽어 agent에 전달한다.

**RL Actor의 역할은 좋은 행동 데이터를 만드는 것이지, VLA의 obs 조건을 모방하는 것이 아니다.** VLA Student는 나중에 (camera image + 9D state + text instruction)만으로 이 행동을 따라해야 한다. 데이터 수집 시 저장하는 것은 (카메라 이미지, 9D state, 9D action, instruction text)뿐이다.

**Teacher 과의존 방지(Visibility Gate)**: RL Actor가 privileged obs에 과도하게 의존하면, 카메라에서 물체가 안 보이는 상황에서도 완벽한 행동을 만들어내고, VLA가 이런 "눈 감고 잡는" 행동을 모방할 수 없다. rollout 데이터 저장 시 **visibility gate**를 적용한다: 물체가 양쪽 카메라 모두에서 보이지 않는 프레임이 존재하는 에피소드의 **앞부분을 잘라내고**, 첫 visible frame부터 저장을 시작한다. 에피소드 중간에 구멍을 내면 action chunk label의 temporal continuity가 깨지므로, 중간 프레임 제거는 하지 않는다.

**PPO 핵심 하이퍼파라미터**: `lr=3e-4` (KLAdaptiveLR 스케줄러 적용 — `kl_threshold=0.01`, BC fine-tune 시 `lr × 0.3 = 9e-5`), `mini_batches=4`, `learning_epochs=5`, `rollouts=24`, `ratio_clip=0.15`, `grad_norm_clip=0.5`, `entropy_coef=0.01`, `clip_predicted_values=True`, `value_clip=0.2`, `value_loss_scale=1.0`, `lambda=0.95`, `gamma=0.99`. **Navigate 전용 변경**: `entropy_coef=0.02` (3D base 탐색에 더 많은 exploration 필요), `clip_predicted_values=False`.

**Audit v2 주요 수정사항 (3건)**: (1) Handoff 좌표계 수정 — `generate_handoff_buffer.py`에서 env_origin 기준 상대 좌표로 저장, `_reset_from_handoff`에서 env_origins를 더해 절대 좌표로 복원, `home_pos_w`를 `env_origins[:, 0:2]`로 설정, (2) `_action_delay_buf` 리셋 — `_finish_reset`에서 delay buffer를 zero로 초기화하여 이전 에피소드 action 잔류 방지, (3) curriculum window 클리어 — 난이도 상승 직후 `_curriculum_success_window.zero_()` + `_curriculum_idx = 0`으로 연쇄 상승 방지.

### 7-2. Handoff Buffer

Skill-2(ApproachAndGrasp)가 끝나면 Skill-3(CarryAndPlace)이 시작된다. Skill-3을 학습하려면 "물체를 성공적으로 잡은 직후"라는 초기 상태가 필요한데, 이것을 임의로 설정하면 비현실적인 상태에서 시작하게 된다.

Skill-2 학습 중 성공한 에피소드의 종료 시점 전체 상태를 저장한다. Skill-3은 이 버퍼에서 초기 상태를 랜덤 샘플하여 시작한다.

Handoff Buffer의 내용은 env_origin 기준 상대 좌표로 저장되며, 로드 시 destination env의 env_origins를 더해 절대 좌표로 복원한다. sim 내부 reset 용도이며 VLA에 전달되지 않는다. 최소 200~500개의 handoff entry를 확보한다.

**Handoff Noise Injection (2중 구조)**: (1) 버퍼 생성 시(`generate_handoff_buffer.py`) 각 entry에 노이즈 추가 (arm joint ±0.05rad, object xy ±0.02m, base xy ±0.03m, base yaw ±0.1rad). (2) Skill-3 매 에피소드 reset 시(`_reset_from_handoff`) per-load 노이즈 재적용 (arm ±0.02rad, base_pos ±0.01m, base_yaw ±0.02rad). 두 노이즈가 합산되어 동일 entry에서도 매번 다른 초기 상태가 생성된다. 이유: 실제 배포 시 VLA의 Skill-2 출력은 RL Expert보다 부정확하다. Skill-3가 "완벽하지 않은 grasp 상태"에서도 복구/운반할 수 있도록 학습시켜야 한다.

### 7-3. Grasp 메커니즘

기존 v8 코드의 physics-based grasp를 유지한다. gripper가 close이고 물체와 접촉하면 `UsdPhysics.FixedJoint`로 결합한다.

**break_force/torque 설정**: 기본값 `break_force = 30N`, `break_torque = 30Nm`, 에피소드마다 DR로 각각 15~45N/Nm 범위에서 랜덤화 (`dr_grasp_break_force_range`, `dr_grasp_break_torque_range`). 기존 v8의 `grasp_joint_break_force=1e8`(사실상 영구 결합)에서 변경한다. 이유: (1) 영구 결합이면 Skill-3(CarryAndPlace)에서 carry 중 조심하는 행동을 학습하지 않아서 real에서 물체를 떨어뜨린다. (2) 고정값이면 그 값에 과적합하므로 DR로 다양한 grasp 강도를 경험시킨다. 구현은 기존 v8의 FixedJoint attach/detach 패턴에 per-env break_force를 적용하는 형태다. **주의**: Skill-3 `_reset_idx()`에서 `_apply_domain_randomization()`을 `_attach_grasp_fixed_joint_for_envs()` **이전**에 호출해야 DR된 break_force가 joint에 반영된다.

**GRASP timeout**: 기존 v8의 `grasp_timeout_steps=75` (~3초@25Hz)를 유지한다. GRASP phase에서 지정 step 내 grasp 미성공 시 APPROACH로 복귀하여 재시도. Skill-2 환경에서 동일하게 적용한다.

**Grasp Break 감지**: FixedJoint가 break_force 초과로 파손되어도 `object_grasped` 플래그는 자동으로 False가 되지 않는다. 매 step gripper body와 물체 사이 거리를 체크하여 `grasp_drop_detect_dist`(0.15m) 초과 시 drop으로 판정한다 (`just_dropped=True`, `object_grasped=False`). Skill-3에서 drop 발생 시 에피소드를 즉시 terminated로 종료하고 `rew_drop_penalty=-10`을 적용한다. 이를 통해 에이전트는 "운반 중 물체를 떨어뜨리면 큰 벌점 + 즉시 종료"를 학습한다. 의도적 place와 비의도적 drop은 Skill-3의 `_update_grasp_state()` 오버라이드로 구분한다. gripper가 `place_gripper_threshold`(0.3) 이상으로 열리고 home 근처(`return_thresh` 내)이면 `intentional_placed=True`로 설정하여 FixedJoint를 해제하고 `just_dropped=False`를 유지한다(place 보상 +20). 반면 break_force 초과로 물체가 떨어진 경우 `just_dropped=True`가 되어 drop penalty(-10) + 즉시 terminated가 된다. Skill-3의 `_get_dones()`는 부모(Skill-2)의 lift 기반 `task_success`를 사용하지 않고, `place_success`(물체가 home 근처에 놓이고 + gripper open + drop이 아닌 경우)를 직접 계산하여 `task_success`를 오버라이드한다. 이는 Skill-3가 handoff buffer에서 이미 grasped+lifted 상태로 시작하기 때문이다.

### 7-3b. Reward 설계 개선

**Action smoothness penalty (-0.005 × Σ(action_delta²))** — 실기 부드러운 동작. 연속 스텝 간 action 변화량 제곱의 **9D 합산**에 비례한 페널티로, 급격한 관절 움직임과 기반부 방향 전환을 억제하여 실제 로봇의 서보 부하와 진동을 줄인다.

**Tanh proximity kernel (2.0 × (1 - tanh(dist/0.5)))** — 목표 근처 강한 gradient. 단순 선형 거리 보상 대신 tanh 기반 커널을 사용하여, 물체/목표까지 거리가 0.5m 이내에서 급격히 증가하는 보상 gradient를 형성한다. 멀리 있을 때는 완만하게, 근접 시 집중적으로 학습하는 효과.

### 7-4. Domain Randomization

기존 v8의 **dynamics DR**을 유지하면서, 데이터 수집 시 **visual DR**을 추가한다.

**Dynamics DR (RL 학습 시, reset-time)**:
- Wheel: stiffness(0.75~1.5x), damping(0.3~3.0x), friction(0.7~1.3x)
- Arm: stiffness(0.8~1.25x), damping(0.5~2.0x)
- Object: mass(0.5~2.0x), friction(0.6~1.5x)
- Observation noise: joint_pos(0.01 rad), base_vel(0.02 m/s), object_rel(0.02 m), lidar(0.03 normalized, Navigate 전용) — 센서 노이즈 시뮬레이션
- Action delay: 1 step (통신 지연 시뮬레이션, SSH/ZMQ 10-50ms)
- Base values: `tuned_dynamics.json`의 `best_params`에서 중심값 결정
- Navigate 전용: wheel DR만 적용 (arm/object/grasp DR 불필요 — arm 고정, 물체 접촉 없음)

이것은 기존 v8 코드(`lekiwi_nav_env.py`의 `enable_domain_randomization=True`)에 이미 구현되어 있으며, sim2real에서 dynamics gap을 줄이는 핵심 요소다.

**Visual DR (데이터 수집 시, 추가)**:
- 조명: 색온도, 강도, 방향 랜덤
- 카메라 노이즈: Gaussian noise + position jitter
- 텍스쳐: 바닥/벽 랜덤 교체
- Distractor 물체: 1~3개 추가

실험 매트릭스에서 C4(Weak DR = dynamics만) vs C5(Strong DR = dynamics + visual) 로 효과를 비교한다.

---

## 8. Navigate 데이터: RL Expert rollout + 스크립트 정책 fallback

Navigate 데이터는 두 가지 방법으로 생성할 수 있다. **주력은 RL Expert rollout** (`collect_demos.py --skill navigate`)이며, 기존 스크립트 정책 (`collect_navigate_data.py`)은 fallback으로 유지된다.

### 8-1. RL Expert (권장)

`lekiwi_skill1_env.py`로 학습한 Navigate RL Expert를 `collect_demos.py --skill navigate`로 rollout한다. RL Expert는 장애물 회피와 목표 근처 감속 정지를 학습했으므로, 스크립트 정책보다 풍부한 행동 패턴(회피 기동, 부드러운 정지)을 포함한 데이터를 생성한다.

arm은 TUCKED_POSE 고정, gripper는 open(1.0) 고정. 9D action에서 arm[0:5]=0.0, grip[5]=1.0으로 오버라이드하여 저장.

### 8-1b. 스크립트 정책 (fallback)

`collect_navigate_data.py`의 proportional controller 기반 데이터 생성도 유지된다.

**두 가지 행동 패턴**: 목표 방향 이동(P-control)과 탐색 회전(랜덤). **노이즈 주입**: 조향 흔들림(σ=0.05 rad), 속도 양자화(3~5단계), 5% action repeat.

### 8-2. 공통 수집 원칙

매 에피소드마다 시작 위치, 목표 위치, 물체 배치를 랜덤화. (이미지, 9D state, 9D action, instruction)을 저장.

**instruction 생성의 가시성 조건**: "search" instruction은 목표 물체가 카메라에 보이지 않을 때만 생성. 물체가 보이는데 "search" instruction이 붙으면 시각-언어 정합성이 깨진다.

**Navigate 환경 요구사항**: Skill-2/3은 평면 위 물체 + 로봇이면 최소 환경이 되지만, Navigate는 시각적 다양성이 필요하다. 최소한 바닥/벽 텍스쳐 DR + distractor 가구 배치로 빈 평면이 아닌 실내 느낌을 만들어야 VLA가 실제 환경에서 일반화할 수 있다.

---

## 9. VLA 모델 선택지

### 9-1. π0-FAST (~3B)

PaliGemma VLM backbone + FAST tokenizer(DCT 기반 action 압축). LeRobot v3 포맷 직접 호환. 정규화는 1st/99th percentile quantile → [-1, 1]. 파인튜닝 VRAM ~25GB, 추론 ~8GB. Action chunk 10 steps.

### 9-2. GR00T N1.6 3B

Cosmos-Reason-2B VLM backbone + 32-layer DiT. state-relative action default. LeRobot v3 직접 못 읽고, 자체 "GR00T-flavored LeRobot v2" + modality.json 필요. Action chunk 16 steps. VRAM 25~48GB.

### 9-3. 선택 전략

실험 C1~C6은 π0-FAST로. C7에서 GR00T N1.6과 비교.

---

## 10. VLA 파인튜닝: 통합 학습

모든 skill의 데이터를 합쳐서 하나의 VLA를 학습한다. Navigate + ApproachAndGrasp + CarryAndPlace 데이터를 하나의 데이터셋으로 합치고, 단일 VLA에 파인튜닝한다. text instruction이 skill을 자연스럽게 분리하기 때문이다.

skill별 모델 분리는 fallback. 통합 모델 성능이 부족할 때만 고려한다.

---

## 11. 데이터 포맷

### 11-1. π0-FAST용: LeRobot v3

Parquet 파일에 state/action 시계열, MP4에 카메라 영상. meta 폴더에 info.json, stats.json, tasks.parquet. 채널명과 순서는 yubinnn11/lekiwi3 (v3.0)과 동일하게 맞춘다.

기존 `convert_hdf5_to_lerobot_v3.py`를 사용하되, robot_state 추출 로직을 새 skill env에 맞게 수정해야 한다. 기존 v8 코드는 `obs[18:24](arm_joint_pos) + obs[30:33](wheel_angular_vel)`을 추출했지만, 이것은 v8의 37D obs 전용이고 wheel_angular_vel ≠ body_velocity이다. 새 skill env에서는 arm_joint_pos(6D) + body_frame_velocity(3D, m/s·rad/s)를 직접 구성하여 `robot_state` 필드에 저장하므로, 변환 스크립트는 이 필드를 그대로 읽으면 된다. 단위 변환(m→mm 등) 불필요.

### 11-2. GR00T용: LeRobot v2 + modality.json

modality.json에 state/action의 semantic 분리, action representation(relative/absolute), action horizon 16 steps, video modality 등을 지정한다.

---

## 12. Sim-Real 캘리브레이션

sim에서 만든 데이터가 real에서 통하려면 물리가 맞아야 한다.

### 12-1. Dynamics 캘리브레이션 ✅ 완료

실제 모터의 마찰, 감속비, 관성을 측정하여 sim 물리 파라미터와 일치시켰다.

**현재 진행 현황 (2026-02-18):**
- 기준 파일: `calibration/calibration_latest.json`
  - `timestamp`: 2026-02-18 10:12:18
  - `connection_mode`: direct, `robot_port`: /dev/ttyACM0
  - `client_id`: my_awesome_kiwi
- Geometry: `lekiwi_robot_cfg.py`의 `WHEEL_RADIUS`/`BASE_RADIUS` config 값 사용
  - wheel_radius=0.049m, base_radius=0.1085m
- Arm joint ranges (실측 완료):
  - shoulder_pan: [-99.48, 100.0]
  - shoulder_lift: [-100.0, 100.0]
  - elbow_flex: [-100.0, 98.93]
  - wrist_flex: [-100.0, 99.92]
  - wrist_roll: [-95.80, 90.43]
  - gripper: [0.075, 100.0]
- Tuning RMSE: wheel=0.117299, arm=0.086990 (gate pass)
- Replay RMSE: wheel=0.145975, arm=0.087000 (gate pass: wheel<0.15, arm<0.09)
- Command transform: LIN_SCALE=1.0166, ANG_SCALE=1.2360, WZ_SIGN=-1.0, LINEAR_MAP=identity

변환식:
- real → sim: `vx_sim = 1.0166 × vx_real`, `vy_sim = 1.0166 × vy_real`, `wz_sim = -1.2360 × wz_real`
- sim → real (배포 시): `vx_real = vx_sim / 1.0166`, `vy_real = vy_sim / 1.0166`, `wz_real = wz_sim / (-1.2360)`

WZ_SIGN이 -1.0인 이유: sim과 real의 회전 양의 방향(좌표계)이 반대다.

### 12-2. 카메라 캘리브레이션 ⬜ 미완

base_cam(D455)의 extrinsic(로봇 body 대비 마운팅 위치/각도) 실측 필요. D455 factory calibration으로 intrinsic은 SDK에서 바로 읽을 수 있다. wrist_cam(USB 웹캠)은 intrinsic/extrinsic 모두 실측 필요.

**USD 카메라 prim path (Isaac Sim 5.0.0 확인)**:
- base_cam: `/World/LeKiwi/base_plate_layer1_v5/Realsense/RSD455/Camera_OmniVision_OV9782_Color`
- wrist_cam: `/World/LeKiwi/Wrist_Roll_08c_v1/visuals/mesh_002_3/wrist_camera`

### 12-3. Joint Limits — 리더암 TCP 측정 → JSON 생성 → USD PhysX 반영

**측정 방식**: Windows에서 `isaac_teleop.py`(리더암 + TCP), Home PC에서 `calibrate_arm_limits.py`를 실행. `isaac_teleop.py`가 leader → sim 좌표 변환(SIGNS 포함)을 하므로, 측정값은 sim 좌표계 기준이며 리더암만 측정하면 됨. 관절별로 양방향 끝까지 밀어 min/max를 기록하고 `calibration/arm_limits_measured.json`에 저장.

```bash
# Home PC
python calibrate_arm_limits.py --port 15002

# Windows (동시 실행)
python isaac_teleop.py
```

RL 학습 시 **제어 target clamp와 USD PhysX joint limit 양쪽 모두에 적용**한다 (`arm_limit_write_to_sim=True`). 텔레옵 시에는 `arm_limit_write_to_sim=False`로 설정하여 PhysX 제약을 해제한다 (USD 기본 리밋 사용, 그리퍼 완전 닫힘 등 허용).

**검증 결과 (Isaac Sim 5.0.0, 2026-02-19)**: LeKiwi USD의 전체 39개 revolute joint이 모두 `(-inf, +inf)`로 확인됨 — arm 6개 + gripper 1개 + wheel 3개 + roller 30개 (단, wheel/roller는 무제한 회전이 물리적으로 정상).

**주의: wrist_roll의 SIGNS=1.814** — `isaac_teleop.py`에서 wrist_roll을 1.814배 증폭한다. 이유: 실물 wrist_roll에 약 1.8:1 기어 증폭이 있어서 서보 1rad 회전 시 실제 손목이 ~1.8rad 회전한다. USD 관절은 기어비를 모델링하지 않으므로 SIGNS로 보상한다. **측정 방법**: SIGNS=1.0 상태에서 그리퍼를 좌→우(180°) 회전시키고 leader_raw 변화량(Δ)을 읽어 `SIGNS = π / Δ`로 계산 (실측값: π / 1.7321 = 1.814). 캘리브레이션 스크립트가 TCP 데이터를 그대로 받으므로 1.814x가 자동 반영된 sim 좌표 기준으로 측정됨.

### 12-4. TUCKED_POSE — 리더암 TCP 측정 (Joint Limits 이전에 수행)

sim 기본 자세(REST_POSE ≈ all-zeros)는 팔이 애매하게 펴진 상태다. **Self-collision이 USD에 설정되어 있지 않아서**, 관절이 움직이면 팔이 몸체를 관통한다. TUCKED_POSE는 팔을 최대한 접은 상태의 관절 값으로, 관절별 self-collision 방지 한계로 사용된다. 각 관절은 tucked pose 값을 넘어서 더 접힐 수 없도록 제한된다.

**측정 방식**: `calibrate_tucked_pose.py` + `isaac_teleop.py`(리더암 TCP). Joint Limits 측정 **이전에** 수행한다 — tucked pose가 관절 가동 범위의 한쪽 경계가 되기 때문.

```bash
# Home PC
python calibrate_tucked_pose.py --port 15002

# Windows (동시 실행)
python isaac_teleop.py
```

```python
TUCKED_POSE = [-0.02966, -0.213839, 0.09066, 0.120177, 0.058418, -0.201554]  # rad (2026-02-21 측정)
# shoulder_pan, shoulder_lift, elbow_flex, wrist_flex, wrist_roll, gripper
```

용도: (1) Navigate skill에서 arm 고정 target, (2) RL 에피소드 초기 arm 자세, (3) self-collision 방지 관절 한계.

### 12-5. ~~lekiwi_v6 데이터 형식 확인~~ ✅ 확정

실제 로봇 데이터(`yubinnn11/lekiwi3`, v3.0) 확인 완료. base state는 body-frame velocity(m/s, rad/s). 채널명: `x.vel, y.vel, theta.vel`. 단위 변환 불필요.

### 12-6. Isaac Sim 5.0.0 API 검증 ✅ 완료 (2026-02-19)

코드 작성에 필요한 Isaac Sim/Lab API의 존재 여부와 동작을 실제 환경에서 검증했다.

**velocity API**: `robot.data.root_lin_vel_b` (body-frame linear, shape `(N,3)`)와 `robot.data.root_ang_vel_b` (body-frame angular, shape `(N,3)`) 모두 존재. world-frame → body-frame 수동 변환 없이 직접 사용 가능.

**body API**: `robot.data.body_pos_w` shape `(N, 40, 3)` (3D tensor). `robot.find_bodies(["Moving_Jaw_08d_v1"])` → `ids=[39]`. USD Inspector에서의 body 순서(index 6)와 Isaac Lab runtime의 순서(index 39)가 다르므로, **반드시 `find_bodies()`로 동적 취득**해야 한다.

**breakForce**: `UsdPhysics.Joint(prim).CreateBreakForceAttr(30.0)` ✅ 정상 작동. `PhysxSchema.PhysxJointAPI.CreateBreakForceAttr()` ❌ 존재하지 않음. v8 코드(L870)에서 이미 올바른 API 사용 중.

**카메라 prim path**: base_cam = `.../Realsense/RSD455/Camera_OmniVision_OV9782_Color`, wrist_cam = `.../Wrist_Roll_08c_v1/visuals/mesh_002_3/wrist_camera`.

---

## 13. 실배포

### 13-1. 추론 아키텍처

Jetson Orin Nano Super가 로봇에 탑재. 역할: D455에서 RGB+Depth+IMU 수신, VIO+Wheel Odometry+EKF로 pose 계산, Safety Layer 실행(100Hz), 카메라 이미지 + 9D state + pose estimate를 WiFi로 A100 서버에 전송.

A100: Qwen2.5-VL-7B가 instruction 생성, VLA가 action chunk 생성, Jetson으로 반환. Jetson은 sim→real 역변환 후 모터에 명령.

**Action Chunk 로컬 버퍼**: WiFi 지연 시에도 끊기지 않도록 Jetson이 chunk를 로컬 버퍼에 저장하고 open-loop 실행. N프레임(15프레임 = 1.5초 @10Hz) 연속 미수신 시 gentle stop.

### 13-2. Sim→Real Action Transform

```
base: vx_real = vx_sim / 1.0166, vy_real = vy_sim / 1.0166, wz_real = wz_sim / (-1.2360)
arm: 그대로 사용 (dynamics calibration으로 일치)
gripper: 그대로
```

기존 `deploy_vla_action_bridge.py`로 VLA action(9D) → sim denorm → sim_to_real → arm limits 매핑의 전체 변환 체인 검증 가능:

```bash
python deploy_vla_action_bridge.py \
  --action "0.2,0.0,-0.3,0,0,0,0,0,0" \
  --action_format v6 \
  --dynamics_json calibration/tuned_dynamics.json \
  --arm_limit_json calibration/arm_limits_real2sim.json \
  --arm_action_to_limits --json
```

### 13-3. Safety Layer (Jetson, 100Hz)

D455 depth 중앙 영역 min < 30cm → emergency stop. 이동 방향 영역 < 20cm → stop. 서버 응답 2초 초과 → gentle stop. 모터 stall 80% 이상 25ms → release.

D455의 depth FOV는 전방 ~86°만 커버. 측면/후방 이동 속도는 보수적으로 제한하고, 필요 시 RPLiDAR A1 추가.

---

## 14. 실험 설계

7개 조건. 모든 조건에서 orchestrator VLM은 Qwen2.5-VL-7B 고정.

C1: 사람 텔레옵 50개 only → π0-FAST. IL-only baseline.
C2: 텔레옵 증강(Isaac Lab Mimic) 1K개 → π0-FAST.
C3: RL expert rollout 1K개 → π0-FAST.
C4: Mimic + RL 합산 1K개 → π0-FAST.
C5: Mimic + RL 합산 1K개 + Strong DR(dynamics + visual) → π0-FAST.
C6: Mimic + RL 합산 10K개 + Strong DR → π0-FAST.
C7: C6과 동일 데이터 → GR00T N1.6.

평가: 실제 로봇, 조건당 최소 30회 시행, 95% binomial CI. 실패 유형을 navigation miss / grasp fail / drop during carry / placement fail로 분류.

---

## 15. 인프라

### A100 서버 (40GB)
- conda inference: Qwen2.5-VL-7B-Instruct 추론 (~15GB VRAM)
- conda rl_train: Isaac Sim 5.0.0.0 (headless) + Isaac Lab v2.2.0 + skrl 1.4.3 (BC/RL 학습)
- conda lerobotpi0: π0-FAST + LeRobot 0.4.3
- conda groot: GR00T N1.6 fine-tuning
- VRAM budget: Qwen2.5-VL ~15GB + VLA 추론 ~8GB = ~23GB (Phase 5 배포 시)
- **서버 설치/전송/검증/학습 가이드**: `feedback/server_guide.md`
- 환경 설치: `feedback/setup_server_env.sh` → 검증: `bash feedback/setup_server_env.sh verify`

### RTX 3090 Desktop (24GB)
- Isaac Sim 5.0 + Isaac Lab 0.44.9
- conda env_isaaclab (Python 3.11, PyTorch 2.7.0+cu128)
- skrl 1.4.3, rsl_rl
- ROS2 Humble
- 1030개 물체 USD + SpawnManager / object_catalog.json (대표 12종)
- 역할: 텔레옵 수집 + VLA 데이터 수집 (카메라 렌더링, RT Core 필요)

### Jetson Orin Nano Super
- 환경 설정 ⬜
- Safety Layer 구현 ⬜
- D455 VIO + Wheel Odometry + EKF Fusion ⬜
- (선택) RPLiDAR A1 360° 장애물 감지 ⬜

---

## 16. 실행 순서 체크리스트

```
Phase 0: Sim-Real 일치 ★ Phase 1 시작 전 필수 (Hard Gate) ★
  [✅] TUCKED_POSE 측정 완료 (`calibrate_tucked_pose.py`, 2026-02-21, calibration/tucked_pose.json)
  [✅] Arm joint limits 측정 완료 (`calibrate_arm_limits.py`, 2026-02-21, calibration/arm_limits_measured.json, tucked pose 제약 적용됨)
  [✅] wrist_roll 기어비 측정 완료 (SIGNS[4]=1.814, isaac_teleop.py 반영)
  [✅] Dynamics 캘리브레이션 (calibration_latest.json, tuned_dynamics.json)
  [✅] Calibration gate 통과 (wheel=0.146 < 0.15, arm=0.087 < 0.09)
  [✅] Command transform 확정 (LIN_SCALE=1.0166, ANG_SCALE=1.2360, WZ_SIGN=-1.0)
  [⬜] 카메라 캘리브레이션 (D455 extrinsic + wrist_cam intrinsic/extrinsic)
  [✅] Joint limits USD PhysX 반영 → RL: `arm_limit_write_to_sim=True`, 텔레옵: `False`
  [✅] 데이터셋 형식 확인 — yubinnn11/lekiwi3 (v3.0), velocity(m/s, rad/s)

Phase 1: RL Expert 학습 (텔레옵: 3090 Desktop, BC/RL: A100 서버)
  [✅] v8 단일 환경 → Skill-1/2/3 분리 환경 리팩토링 (lekiwi_skill1_env.py, lekiwi_skill2_env.py, lekiwi_skill3_env.py + AAC 3파일)
  [✅] Skill-1 Navigate 환경 구현 (lekiwi_skill1_env.py: 20D actor, 25D critic, 텐서 기반 장애물, pseudo-lidar)
  [⬜] A100 서버 환경 설치 (feedback/setup_server_env.sh)
  [⬜] Skill-1 RL (A100, from scratch PPO, BC 불필요)
  [⬜] Skill-2 텔레옵 10~20개 (3090 Desktop)
  [⬜] Skill-2 BC 학습 (A100, 30D obs)
  [⬜] Skill-2 RL (A100, BC→PPO, 성공률 90%+)
  [⬜] Handoff Buffer 생성 (200~500개)
  [⬜] Skill-3 텔레옵 10~20개 (3090 Desktop, Handoff Buffer 상태에서)
  [⬜] Skill-3 BC 학습 (A100, 29D obs)
  [⬜] Skill-3 RL (A100, BC→PPO, 성공률 90%+)

Phase 2: VLA Training Data 수집 (RTX 3090)
  [⬜] Navigate RL Expert rollout 데이터 수집 (1K~2K개, collect_demos.py --skill navigate)
  [⬜] Skill-2 RL Expert rollout (1K~10K, 성공 + visibility gate)
  [⬜] Skill-3 RL Expert rollout (1K~10K, 성공 + visibility gate)
  [⬜] (선택) Isaac Lab Mimic 증강

Phase 3: 데이터 변환
  [⬜] HDF5 → LeRobot v3
  [✅] base 단위 변환 불필요 확인 (sim velocity m/s = real velocity m/s)
  [⬜] gripper 값 binary 변환 (RL continuous → 0.5 threshold → 0/1)
  [⬜] (GR00T) v3 → v2 + modality.json
  [⬜] 채널명/순서 검증 (yubinnn11/lekiwi3 v3.0 일치: x.vel, y.vel, theta.vel)

Phase 4: VLA 파인튜닝 (A100)
  [⬜] Navigate + Skill-2 + Skill-3 데이터 통합
  [⬜] π0-FAST fine-tune
  [⬜] (선택) GR00T N1.6 fine-tune
  [⬜] Sim inference sanity check

Phase 5: 실배포
  [⬜] Jetson 환경 설정
  [⬜] D455 VIO + Wheel Odometry + EKF Fusion 구현
  [⬜] Safety Layer 구현
  [⬜] VLM orchestrator system prompt 구축
  [⬜] VLM 루프 구현 (0.3Hz 판단 + VLA instruction 전달)
  [⬜] Sim→Real action transform 적용 (deploy_vla_action_bridge.py --action_format v6 검증)
  [⬜] 실험 C1~C7 실행
```
